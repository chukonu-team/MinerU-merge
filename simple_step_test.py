#!/usr/bin/env python3
"""
åˆ†æ­¥å¤„ç†æµ‹è¯• - ä½¿ç”¨doc_analyzeé€ä¸ªå¤„ç†PDFæ–‡ä»¶
"""
import os
import sys
import time
from pathlib import Path
from datetime import datetime

# æ·»åŠ mineruæ¨¡å—åˆ°è·¯å¾„
sys.path.insert(0, '/home/ubuntu/MinerU')

from mineru.cli.common import convert_pdf_bytes_to_bytes_by_pypdfium2, prepare_env, read_fn
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.backend.vlm.vlm_analyze import doc_analyze
from mineru.utils.guess_suffix_or_lang import guess_suffix_by_path
from demo.demo import _process_output
from mineru.utils.enum_class import MakeMode


def get_pdf_files(demo_dir):
    """è·å–demoç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""
    pdf_files = []
    demo_path = Path(demo_dir)

    if not demo_path.exists():
        print(f"Demoç›®å½•ä¸å­˜åœ¨: {demo_dir}")
        return pdf_files

    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    for pdf_file in demo_path.glob("*.pdf"):
        if pdf_file.is_file():
            pdf_files.append(pdf_file)

    return sorted(pdf_files)


def main():
    """åˆ†æ­¥å¤„ç†æµ‹è¯•"""
    print("ğŸš€ åˆ†æ­¥å¤„ç†æµ‹è¯• (doc_analyze)")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    # è®¾ç½®è·¯å¾„
    demo_dir = "/home/ubuntu/MinerU/demo/pdfs"
    output_base_dir = Path("/home/ubuntu/MinerU/batch_vs_step_step")
    output_base_dir.mkdir(exist_ok=True)

    # è·å–PDFæ–‡ä»¶
    pdf_files = get_pdf_files(demo_dir)
    if not pdf_files:
        print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return

    # æµ‹è¯•3ä¸ªæ–‡ä»¶ï¼ˆä¸æ‰¹é‡å¤„ç†æµ‹è¯•ç›¸åŒï¼‰
    max_files = min(3, len(pdf_files))
    test_pdf_files = pdf_files[:max_files]

    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶ ({max_files}ä¸ª):")
    total_size = 0
    for i, pdf_file in enumerate(test_pdf_files, 1):
        file_size = pdf_file.stat().st_size / 1024 / 1024  # MB
        total_size += file_size
        print(f"  {i}. {pdf_file.name} ({file_size:.2f} MB)")
    print(f"æ€»å¤§å°: {total_size:.2f} MB")

    # ä½¿ç”¨vlm-vllm-engineåç«¯
    backend = "vlm-vllm-engine"

    try:
        # è®°å½•æ€»å¼€å§‹æ—¶é—´
        total_start_time = time.time()

        # ç»Ÿè®¡å˜é‡
        total_pages_processed = 0
        total_files_generated = 0
        processing_times = []

        print(f"\nğŸ”„ å¼€å§‹åˆ†æ­¥å¤„ç† {len(test_pdf_files)} ä¸ªPDF...")
        print(f"åç«¯: {backend}")

        # é€ä¸ªå¤„ç†PDF
        for idx, pdf_path in enumerate(test_pdf_files):
            print(f"\n{'-'*60}")
            print(f"å¤„ç†PDF {idx+1}/{len(test_pdf_files)}: {pdf_path.name}")
            print(f"{'-'*60}")

            # è®°å½•å•ä¸ªPDFå¤„ç†å¼€å§‹æ—¶é—´
            pdf_start_time = time.time()

            # è¯»å–PDFæ–‡ä»¶
            pdf_bytes = read_fn(pdf_path)
            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
            pdf_file_name = pdf_path.stem

            print(f"ğŸ“– å·²åŠ è½½: {pdf_path.name}")

            # å‡†å¤‡è¾“å‡ºç›®å½•
            local_image_dir, local_md_dir = prepare_env(output_base_dir / pdf_file_name, pdf_file_name, "vlm")
            image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(local_md_dir)

            print(f"ğŸ“ è¾“å‡ºç›®å½•: {local_md_dir}")

            # ä½¿ç”¨doc_analyzeå¤„ç†å•ä¸ªPDF
            print(f"ğŸ¤– å¼€å§‹æ¨ç†å¤„ç†...")
            middle_json, infer_result = doc_analyze(
                pdf_bytes,
                image_writer=image_writer,
                backend=backend[4:],  # å»æ‰"vlm-"å‰ç¼€
                server_url=None
            )

            # å¤„ç†è¾“å‡ºæ–‡ä»¶
            print(f"ğŸ“ ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
            pdf_info = middle_json["pdf_info"]
            _process_output(
                pdf_info, pdf_bytes, pdf_file_name, local_md_dir, local_image_dir,
                md_writer, f_draw_layout_bbox=True, f_draw_span_bbox=False, f_dump_orig_pdf=True,
                f_dump_md=True, f_dump_content_list=True, f_dump_middle_json=True, f_dump_model_output=True,
                f_make_md_mode=MakeMode.MM_MD, middle_json=middle_json, model_output=infer_result, is_pipeline=False
            )

            # è®°å½•å•ä¸ªPDFå¤„ç†ç»“æŸæ—¶é—´
            pdf_end_time = time.time()
            pdf_processing_time = pdf_end_time - pdf_start_time
            processing_times.append(pdf_processing_time)

            # ç»Ÿè®¡ç»“æœ
            pages = len(middle_json["pdf_info"])
            total_pages_processed += pages

            # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡
            if Path(local_md_dir).exists():
                output_files = list(Path(local_md_dir).rglob("*"))
                total_files_generated += len(output_files)
                md_files = [f for f in output_files if f.suffix == '.md']
                json_files = [f for f in output_files if f.suffix == '.json']
                img_files = [f for f in output_files if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]

                print(f"âœ… å¤„ç†å®Œæˆ!")
                print(f"  å¤„ç†æ—¶é—´: {pdf_processing_time:.2f} ç§’")
                print(f"  é¡µæ•°: {pages}")
                print(f"  ç”Ÿæˆæ–‡ä»¶: {len(output_files)} ä¸ª")
                print(f"    - Markdown: {len(md_files)}")
                print(f"    - JSON: {len(json_files)}")
                print(f"    - å›¾ç‰‡: {len(img_files)}")

        # è®°å½•æ€»ç»“æŸæ—¶é—´
        total_end_time = time.time()
        step_processing_time = total_end_time - total_start_time

        print(f"\nâœ… åˆ†æ­¥å¤„ç†å®Œæˆ!")

        print(f"\nğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"  å¤„ç†PDFæ•°é‡: {len(test_pdf_files)}")
        print(f"  æ€»é¡µæ•°: {total_pages_processed}")
        print(f"  æ€»å¤„ç†æ—¶é—´: {step_processing_time:.2f} ç§’")

        # è®¡ç®—å„ç§å¹³å‡å€¼
        avg_time_per_pdf = sum(processing_times) / len(processing_times)
        avg_time_per_page = step_processing_time / total_pages_processed

        print(f"  å¹³å‡æ¯PDF: {avg_time_per_pdf:.2f} ç§’")
        print(f"  å¹³å‡æ¯é¡µ: {avg_time_per_page:.2f} ç§’")
        if total_pages_processed > 0:
            print(f"  å¤„ç†é€Ÿåº¦: {total_pages_processed/step_processing_time:.2f} é¡µ/ç§’")
        print(f"  æ€»ç”Ÿæˆæ–‡ä»¶: {total_files_generated} ä¸ª")

        # æ˜¾ç¤ºæ¯ä¸ªPDFçš„è¯¦ç»†æ—¶é—´
        print(f"\nâ±ï¸ å„PDFå¤„ç†æ—¶é—´:")
        for i, (pdf_file, proc_time) in enumerate(zip(test_pdf_files, processing_times)):
            print(f"  {i+1}. {pdf_file.name}: {proc_time:.2f} ç§’")
        print(f"  æœ€å¿«: {min(processing_times):.2f} ç§’")
        print(f"  æœ€æ…¢: {max(processing_times):.2f} ç§’")
        print(f"  æ–¹å·®: {max(processing_times) - min(processing_times):.2f} ç§’")

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_file = output_base_dir / "step_results.txt"
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write(f"åˆ†æ­¥å¤„ç†æµ‹è¯•ç»“æœ\n")
            f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å¤„ç†æ–‡ä»¶: {', '.join([pdf.name for pdf in test_pdf_files])}\n")
            f.write(f"æ€»å¤„ç†æ—¶é—´: {step_processing_time:.2f} ç§’\n")
            f.write(f"å¹³å‡æ¯PDF: {avg_time_per_pdf:.2f} ç§’\n")
            f.write(f"å¤„ç†é€Ÿåº¦: {total_pages_processed/step_processing_time:.2f} é¡µ/ç§’\n")
            f.write(f"æ€»ç”Ÿæˆæ–‡ä»¶: {total_files_generated} ä¸ª\n\n")
            f.write(f"å„PDFå¤„ç†æ—¶é—´:\n")
            for i, (pdf_file, proc_time) in enumerate(zip(test_pdf_files, processing_times)):
                f.write(f"  {i+1}. {pdf_file.name}: {proc_time:.2f} ç§’\n")

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"ğŸ è¾“å‡ºç›®å½•: {output_base_dir}")
        print(f"ğŸ‰ åˆ†æ­¥å¤„ç†æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ åˆ†æ­¥å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()