#!/usr/bin/env python3
"""ä¿®å¤çš„MinerUè¿›ç¨‹æ±  - ç®€åŒ–ç‰ˆæœ¬"""

import os
import time
import json
import glob
import random
from typing import List, Dict, Any, Optional
import copy
import zipfile
import traceback
import logging

from mineru.cli.common import read_fn, convert_pdf_bytes_to_bytes_by_pypdfium2

# å¯¼å…¥ç®€åŒ–çš„è¿›ç¨‹æ± 
from process_pool import SimpleProcessPool

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [PID:%(process)d][%(thread)d] %(levelname)s: %(message)s"
)


def get_pdf_page_count(pdf_path):
    """ä½¿ç”¨fitzè·å–PDFé¡µæ•°"""
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        page_count = len(doc)
        doc.close()
        return page_count
    except Exception as e:
        logging.warning(f"Error getting page count for {pdf_path}: {e}")
        return 0


def process_batch_pdf_files(batch, save_dir, backend="vllm-engine"):
    start = time.time()
    logging.info(f"æ‰¹å¤„ç†å¼€å§‹")
    result_dir = f"{save_dir}/result"
    if not os.path.exists(result_dir):
        os.makedirs(result_dir, exist_ok=True)

    pdf_bytes_list = []
    image_writers = []
    pdf_paths = batch['files']
    read_start = time.time()
    from mineru.data.data_reader_writer import FileBasedDataWriter
    for i in range(len(pdf_paths) - 1, -1, -1):
        try:
            pdf_bytes = read_fn(pdf_paths[i])
            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
            pdf_bytes_list.append(pdf_bytes)
            pdf_name = os.path.basename(pdf_paths[i])
            local_image_dir = f"/mnt/data/mineru_ocr_local_image_dir/{pdf_name}"
            if not os.path.exists(local_image_dir):
                os.makedirs(local_image_dir, exist_ok=True)
            image_writer = FileBasedDataWriter(local_image_dir)
            image_writers.append(image_writer)
        except Exception as e:
            logging.warning(f"åŠ è½½ {pdf_paths[i]} å¤±è´¥: {e}")
            traceback.print_exc()
            del pdf_paths[i]
    logging.info(f"åŠ è½½å®Œæ¯•ï¼Œè€—æ—¶{time.time() - read_start}")
    from mineru.backend.vlm.vlm_analyze import batch_doc_analyze
    gpu_memory_utilization = os.environ.get("GPU_MEMORY_UTILIZATION", 0.5)
    all_middle_json, _ = batch_doc_analyze(
        pdf_bytes_list=pdf_bytes_list,
        image_writer_list=image_writers,
        backend=backend,
        server_url=None,
        gpu_memory_utilization=gpu_memory_utilization
    )
    results = []
    for pdf_path, middle_json in zip(pdf_paths, all_middle_json):
        pdf_file_name = os.path.basename(pdf_path).replace(".pdf", "")
        if middle_json is not None:
            infer_result = {"middle_json": middle_json}
            res_json_str = json.dumps(infer_result, ensure_ascii=False)
            # ä¿å­˜ä¸ºå‹ç¼©æ–‡ä»¶
            result_dir = f"{save_dir}/result"
            if not os.path.exists(result_dir):
                os.makedirs(result_dir, exist_ok=True)
            target_file = f"{result_dir}/{pdf_file_name}.json.zip"
            with zipfile.ZipFile(target_file, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                res_json_bytes = res_json_str.encode("utf-8")
                zf.writestr(f"{pdf_file_name}.json", res_json_bytes)
            page_count = get_pdf_page_count(pdf_path)
            file_size = os.path.getsize(target_file)
            result = {
                'input_path': pdf_path,
                'output_path': target_file,
                'page_count': page_count,
                'file_size': file_size,
                'success': True
            }
        else:
            result = {
                'input_path': pdf_path,
                'output_path': None,
                'success': False
            }
        page_result_path = f"{save_dir}/page_result"
        if not os.path.exists(page_result_path):
            os.makedirs(page_result_path, exist_ok=True)
        json_file_name = f"{pdf_file_name}.json"
        temp_json_path = os.path.join(page_result_path, json_file_name)
        with open(temp_json_path, 'w') as f:
            json.dump(result, f)
        results.append(result)
    logging.info(f"æ‰¹å¤„ç†ç»“æŸï¼Œè€—æ—¶{time.time() - start}ç§’")
    return results


def preprocessing_task_with_image_loading(batch, save_dir, **kwargs):
    """
    å¢å¼ºç‰ˆé¢„å¤„ç†å‡½æ•° - è¯»å–PDFæ–‡ä»¶ã€è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼ã€åŠ è½½å›¾åƒ
    è¿™éƒ¨åˆ†å·¥ä½œåŒ…å«load_images_from_pdfï¼Œä¸ä¾èµ–GPUï¼Œå¯ä»¥åœ¨CPUä¸Šå¹¶è¡Œå¤„ç†
    """
    start = time.time()
    logging.info(f"å¢å¼ºé¢„å¤„ç†æ‰¹æ¬¡å¼€å§‹: {batch.get('file_names', [])}")

    pdf_bytes_list = []
    image_writers = []
    pdf_paths = batch['files'].copy()

    read_start = time.time()
    from mineru.data.data_reader_writer import FileBasedDataWriter

    # è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼
    for i in range(len(pdf_paths) - 1, -1, -1):
        try:
            pdf_bytes = read_fn(pdf_paths[i])
            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
            pdf_bytes_list.append(pdf_bytes)
            pdf_name = os.path.basename(pdf_paths[i])
            local_image_dir = f"/mnt/data/mineru_ocr_local_image_dir/{pdf_name}"
            if not os.path.exists(local_image_dir):
                os.makedirs(local_image_dir, exist_ok=True)
            image_writer = FileBasedDataWriter(local_image_dir)
            image_writers.append(image_writer)
        except Exception as e:
            logging.warning(f"åŠ è½½ {pdf_paths[i]} å¤±è´¥: {e}")
            traceback.print_exc()
            del pdf_paths[i]

    preprocess_time = time.time() - read_start
    logging.info(f"PDFè¯»å–å®Œæ¯•ï¼Œè€—æ—¶{preprocess_time:.2f}ç§’")

    # åŠ è½½å›¾åƒï¼ˆè¿™éƒ¨åˆ†åŸåœ¨batch_doc_analyzeä¸­ï¼‰
    image_loading_start = time.time()
    from mineru.backend.vlm.vlm_analyze import load_images_from_pdf

    all_images_list = []
    all_pdf_docs = []
    images_count_per_pdf = []
    pdf_processing_status = []

    # éå†æ‰€æœ‰PDFæ–‡æ¡£ï¼ŒåŠ è½½å›¾åƒ
    for pdf_bytes in pdf_bytes_list:
        try:
            images_list, pdf_doc = load_images_from_pdf(pdf_bytes, image_type=ImageType.PIL)
            all_images_list.extend(images_list)
            all_pdf_docs.append(pdf_doc)
            images_count_per_pdf.append(len(images_list))
            pdf_processing_status.append(True)  # æ ‡è®°ä¸ºæˆåŠŸå¤„ç†
        except Exception as e:
            logging.warning(f"ä»PDFåŠ è½½å›¾åƒå¤±è´¥: {e}")
            # æ·»åŠ Noneä½œä¸ºpdf_docï¼Œæ ‡è®°å¤±è´¥çŠ¶æ€
            all_pdf_docs.append(None)
            images_count_per_pdf.append(0)  # å›¾åƒæ•°é‡ä¸º0
            pdf_processing_status.append(False)  # æ ‡è®°ä¸ºå¤„ç†å¤±è´¥

    image_loading_time = time.time() - image_loading_start
    logging.info(f"å›¾åƒåŠ è½½å®Œæ¯•ï¼Œè€—æ—¶{image_loading_time:.2f}ç§’")

    # ç”Ÿæˆæœ‰æ•ˆçš„PILå›¾åƒåˆ—è¡¨
    images_pil_list = []
    for image_dict in all_images_list:
        if image_dict and isinstance(image_dict, dict) and "img_pil" in image_dict:
            images_pil_list.append(image_dict["img_pil"])

    total_preprocess_time = time.time() - start
    logging.info(f"å¢å¼ºé¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶{total_preprocess_time:.2f}ç§’ï¼Œæœ‰æ•ˆå›¾åƒæ•°: {len(images_pil_list)}")

    # è¿”å›é¢„å¤„ç†åçš„æ•°æ®ï¼Œä¾›GPUå‡½æ•°ä½¿ç”¨
    return {
        'pdf_bytes_list': pdf_bytes_list,
        'image_writers': image_writers,
        'pdf_paths': pdf_paths,
        'save_dir': save_dir,
        'preprocess_time': preprocess_time,
        'image_loading_time': image_loading_time,
        'total_preprocess_time': total_preprocess_time,
        'all_images_list': all_images_list,
        'all_pdf_docs': all_pdf_docs,
        'images_count_per_pdf': images_count_per_pdf,
        'pdf_processing_status': pdf_processing_status,
        'images_pil_list': images_pil_list,
        'batch_info': batch
    }


def preprocessing_task(batch, save_dir, **kwargs):
    """
    é¢„å¤„ç†å‡½æ•° - è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼
    è¿™éƒ¨åˆ†å·¥ä½œä¸ä¾èµ–GPUï¼Œå¯ä»¥åœ¨CPUä¸Šå¹¶è¡Œå¤„ç†
    """
    start = time.time()
    logging.info(f"é¢„å¤„ç†æ‰¹æ¬¡å¼€å§‹: {batch.get('file_names', [])}")

    pdf_bytes_list = []
    image_writers = []
    pdf_paths = batch['files'].copy()

    read_start = time.time()
    from mineru.data.data_reader_writer import FileBasedDataWriter

    # è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼
    for i in range(len(pdf_paths) - 1, -1, -1):
        try:
            pdf_bytes = read_fn(pdf_paths[i])
            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
            pdf_bytes_list.append(pdf_bytes)
            pdf_name = os.path.basename(pdf_paths[i])
            local_image_dir = f"/mnt/data/mineru_ocr_local_image_dir/{pdf_name}"
            if not os.path.exists(local_image_dir):
                os.makedirs(local_image_dir, exist_ok=True)
            image_writer = FileBasedDataWriter(local_image_dir)
            image_writers.append(image_writer)
        except Exception as e:
            logging.warning(f"åŠ è½½ {pdf_paths[i]} å¤±è´¥: {e}")
            traceback.print_exc()
            del pdf_paths[i]

    preprocess_time = time.time() - read_start
    logging.info(f"é¢„å¤„ç†åŠ è½½å®Œæ¯•ï¼Œè€—æ—¶{preprocess_time:.2f}ç§’")

    # è¿”å›é¢„å¤„ç†åçš„æ•°æ®ï¼Œä¾›GPUå‡½æ•°ä½¿ç”¨
    return {
        'pdf_bytes_list': pdf_bytes_list,
        'image_writers': image_writers,
        'pdf_paths': pdf_paths,
        'save_dir': save_dir,
        'preprocess_time': preprocess_time,
        'batch_info': batch
    }


def gpu_processing_task_with_preloaded_images(preprocessed_data, **kwargs):
    """
    GPUå¤„ç†å‡½æ•° - ä½¿ç”¨GPUè¿›è¡Œæ–‡æ¡£åˆ†æå¹¶ä¿å­˜ç»“æœï¼ˆåŸºäºé¢„åŠ è½½çš„å›¾åƒï¼‰
    è¿™éƒ¨åˆ†å·¥ä½œéœ€è¦GPUï¼Œåªå¤„ç†batch_two_step_extractåŠä¹‹åçš„éƒ¨åˆ†
    """
    start = time.time()

    # ä»é¢„å¤„ç†æ•°æ®ä¸­è·å–æ‰€æœ‰å¿…è¦ä¿¡æ¯
    all_images_list = preprocessed_data['all_images_list']
    all_pdf_docs = preprocessed_data['all_pdf_docs']
    images_count_per_pdf = preprocessed_data['images_count_per_pdf']
    pdf_processing_status = preprocessed_data['pdf_processing_status']
    images_pil_list = preprocessed_data['images_pil_list']
    save_dir = preprocessed_data['save_dir']
    image_writers = preprocessed_data['image_writers']
    batch = preprocessed_data['batch_info']

    logging.info(f"GPUå¤„ç†å¼€å§‹ï¼ˆåŸºäºé¢„åŠ è½½å›¾åƒï¼‰: {batch.get('file_names', [])}")

    try:
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å›¾åƒï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
        if not images_pil_list:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„å›¾åƒï¼Œè¿”å›ç©ºç»“æœ")
            all_middle_json = [None] * len(all_pdf_docs)
        else:
            # è·å–predictor
            backend = os.environ.get("BACKEND", "vllm-engine")
            logging.info(f"backend: {backend}")

            from mineru.backend.vlm.vlm_analyze import ModelSingleton
            predictor = ModelSingleton().get_model(backend, None, None, **kwargs)

            # GPUæ¨ç† - åªè°ƒç”¨batch_two_step_extract
            gpu_start = time.time()
            results = predictor.batch_two_step_extract(images=images_pil_list)
            gpu_time = time.time() - gpu_start
            logging.info(f"GPUæ¨ç†å®Œæ¯•ï¼Œè€—æ—¶{gpu_time:.2f}ç§’")

            # éœ€è¦ä¸ºæ¯ä¸ªPDFæ–‡æ¡£åˆ†åˆ«ç”Ÿæˆmiddle_json
            all_middle_json = []
            image_idx = 0

            for i, (pdf_doc, is_success) in enumerate(zip(all_pdf_docs, pdf_processing_status)):
                if not is_success or pdf_doc is None:
                    # å¯¹äºå¤„ç†å¤±è´¥çš„PDFï¼Œè¿”å›None
                    all_middle_json.append(None)
                    continue

                # è·å–å½“å‰PDFçš„å›¾åƒæ•°é‡
                current_pdf_images_count = images_count_per_pdf[i]

                if current_pdf_images_count == 0:
                    # å¯¹äºæ²¡æœ‰å›¾åƒçš„PDFï¼Œè¿”å›None
                    all_middle_json.append(None)
                    continue

                # è·å–å½“å‰PDFçš„å›¾åƒåˆ—è¡¨å’Œç»“æœ
                current_images_list = all_images_list[image_idx: image_idx + current_pdf_images_count]
                current_results = results[image_idx: image_idx + current_pdf_images_count]

                # ä¸ºå½“å‰PDFç”Ÿæˆmiddle_json
                from mineru.backend.vlm.vlm_analyze import result_to_middle_json
                image_writer = image_writers[i] if i < len(image_writers) else None
                middle_json = result_to_middle_json(current_results, current_images_list, pdf_doc, image_writer)
                all_middle_json.append(middle_json)

                # æ›´æ–°å›¾åƒç´¢å¼•
                image_idx += current_pdf_images_count

        # ä¿å­˜ç»“æœ
        final_results = []
        for i, (middle_json, is_success) in enumerate(zip(all_middle_json, pdf_processing_status)):
            if not is_success:
                # å¯¹äºé¢„å¤„ç†å¤±è´¥çš„PDFï¼Œè·³è¿‡ä¿å­˜
                continue

            # ä»åŸå§‹batchä¿¡æ¯ä¸­è·å–PDFè·¯å¾„
            if 'files' in batch and i < len(batch['files']):
                pdf_path = batch['files'][i]
            else:
                continue

            pdf_file_name = os.path.basename(pdf_path).replace(".pdf", "")

            if middle_json is not None:
                infer_result = {"middle_json": middle_json}
                res_json_str = json.dumps(infer_result, ensure_ascii=False)

                # ä¿å­˜ä¸ºå‹ç¼©æ–‡ä»¶
                result_dir = f"{save_dir}/result"
                if not os.path.exists(result_dir):
                    os.makedirs(result_dir, exist_ok=True)
                target_file = f"{result_dir}/{pdf_file_name}.json.zip"
                with zipfile.ZipFile(target_file, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                    res_json_bytes = res_json_str.encode("utf-8")
                    zf.writestr(f"{pdf_file_name}.json", res_json_bytes)

                page_count = get_pdf_page_count(pdf_path)
                file_size = os.path.getsize(target_file)
                result = {
                    'input_path': pdf_path,
                    'output_path': target_file,
                    'page_count': page_count,
                    'file_size': file_size,
                    'success': True
                }
            else:
                result = {
                    'input_path': pdf_path,
                    'output_path': None,
                    'success': False
                }

            # ä¿å­˜é¡µé¢ç»“æœä¿¡æ¯
            page_result_path = f"{save_dir}/page_result"
            if not os.path.exists(page_result_path):
                os.makedirs(page_result_path, exist_ok=True)
            json_file_name = f"{pdf_file_name}.json"
            temp_json_path = os.path.join(page_result_path, json_file_name)
            with open(temp_json_path, 'w') as f:
                json.dump(result, f)
            final_results.append(result)

        total_time = time.time() - start
        logging.info(f"GPUå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’")

        return {
            'success': True,
            'results': final_results,
            'preprocess_time': preprocessed_data.get('preprocess_time', 0),
            'image_loading_time': preprocessed_data.get('image_loading_time', 0),
            'gpu_time': gpu_time if 'gpu_time' in locals() else 0,
            'total_preprocess_time': preprocessed_data.get('total_preprocess_time', 0),
            'total_time': total_time,
            'batch_info': batch
        }

    except Exception as e:
        error_time = time.time() - start
        logging.error(f"GPUå¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'error_time': error_time,
            'batch_info': batch
        }


def gpu_processing_task(preprocessed_data, **kwargs):
    """
    GPUå¤„ç†å‡½æ•° - ä½¿ç”¨GPUè¿›è¡Œæ–‡æ¡£åˆ†æå¹¶ä¿å­˜ç»“æœ
    è¿™éƒ¨åˆ†å·¥ä½œéœ€è¦GPUï¼Œå¤„ç†é¢„å¤„ç†åçš„æ•°æ®
    """
    start = time.time()
    pdf_bytes_list = preprocessed_data['pdf_bytes_list']
    image_writers = preprocessed_data['image_writers']
    pdf_paths = preprocessed_data['pdf_paths']
    save_dir = preprocessed_data['save_dir']
    batch = preprocessed_data['batch_info']

    logging.info(f"GPUå¤„ç†å¼€å§‹: {batch.get('file_names', [])}")

    try:
        backend = os.environ.get("BACKEND", "vllm-engine")
        logging.info(f"backend: {backend}")

        # è°ƒç”¨batch_doc_analyzeè¿›è¡ŒGPUåˆ†æ
        from mineru.backend.vlm.vlm_analyze import batch_doc_analyze
        gpu_memory_utilization = os.environ.get("GPU_MEMORY_UTILIZATION", 0.5)

        gpu_start = time.time()
        all_middle_json, _ = batch_doc_analyze(
            pdf_bytes_list=pdf_bytes_list,
            image_writer_list=image_writers,
            backend=backend,
            server_url=None,
            gpu_memory_utilization=gpu_memory_utilization
        )
        gpu_time = time.time() - gpu_start
        logging.info(f"GPUåˆ†æå®Œæ¯•ï¼Œè€—æ—¶{gpu_time:.2f}ç§’")

        # ä¿å­˜ç»“æœ
        results = []
        for pdf_path, middle_json in zip(pdf_paths, all_middle_json):
            pdf_file_name = os.path.basename(pdf_path).replace(".pdf", "")
            if middle_json is not None:
                infer_result = {"middle_json": middle_json}
                res_json_str = json.dumps(infer_result, ensure_ascii=False)

                # ä¿å­˜ä¸ºå‹ç¼©æ–‡ä»¶
                result_dir = f"{save_dir}/result"
                if not os.path.exists(result_dir):
                    os.makedirs(result_dir, exist_ok=True)
                target_file = f"{result_dir}/{pdf_file_name}.json.zip"
                with zipfile.ZipFile(target_file, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                    res_json_bytes = res_json_str.encode("utf-8")
                    zf.writestr(f"{pdf_file_name}.json", res_json_bytes)

                page_count = get_pdf_page_count(pdf_path)
                file_size = os.path.getsize(target_file)
                result = {
                    'input_path': pdf_path,
                    'output_path': target_file,
                    'page_count': page_count,
                    'file_size': file_size,
                    'success': True
                }
            else:
                result = {
                    'input_path': pdf_path,
                    'output_path': None,
                    'success': False
                }

            # ä¿å­˜é¡µé¢ç»“æœä¿¡æ¯
            page_result_path = f"{save_dir}/page_result"
            if not os.path.exists(page_result_path):
                os.makedirs(page_result_path, exist_ok=True)
            json_file_name = f"{pdf_file_name}.json"
            temp_json_path = os.path.join(page_result_path, json_file_name)
            with open(temp_json_path, 'w') as f:
                json.dump(result, f)
            results.append(result)

        total_time = time.time() - start
        logging.info(f"GPUå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’")

        return {
            'success': True,
            'results': results,
            'preprocess_time': preprocessed_data['preprocess_time'],
            'gpu_time': gpu_time,
            'total_time': total_time,
            'batch_info': batch
        }

    except Exception as e:
        error_time = time.time() - start
        logging.error(f"GPUå¤„ç†å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'error_time': error_time,
            'batch_info': batch
        }


def gpu_worker_task(batch, save_dir, **kwargs):
    """
    GPUå·¥ä½œè¿›ç¨‹çš„ä»»åŠ¡å‡½æ•° - é€‚é…åŒç¼“å†²ç³»ç»Ÿ
    ç°åœ¨è¿™ä¸ªå‡½æ•°è°ƒç”¨å¢å¼ºé¢„å¤„ç†ä»»åŠ¡ï¼ˆåŒ…å«å›¾åƒåŠ è½½ï¼‰ï¼Œé¢„å¤„ç†ç»“æœä¼šè¢«æ”¾å…¥GPUé˜Ÿåˆ—
    """

    logging.info(f"GPU worker task å¼€å§‹å¢å¼ºé¢„å¤„ç†: {batch.get('file_names', [])}")
    try:
        # æ‰§è¡Œå¢å¼ºé¢„å¤„ç†ä»»åŠ¡ï¼ˆåŒ…å«å›¾åƒåŠ è½½ï¼‰
        preprocessed_data = preprocessing_task_with_image_loading(batch, save_dir, **kwargs)
        logging.info(f"å¢å¼ºé¢„å¤„ç†å®Œæˆï¼Œå‡†å¤‡æäº¤åˆ°GPUé˜Ÿåˆ—")
        return preprocessed_data
    except Exception as e:
        logging.error(f"å¢å¼ºé¢„å¤„ç†å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'batch_info': batch
        }


def create_batches_by_pages(pdf_files, batch_size, output_path, max_pages_per_pdf=1000):
    """
    æ ¹æ®é¡µæ•°åˆ›å»ºæ‰¹æ¬¡
    :param pdf_files: PDFæ–‡ä»¶åˆ—è¡¨
    :param batch_size: æ¯æ‰¹æ¬¡æœ€å¤§é¡µæ•°
    :return: æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«æ–‡ä»¶åˆ—è¡¨å’Œæ€»é¡µæ•°
    """
    batches = []
    current_batch = []
    current_batch_pages = 0

    logging.info(f"ğŸ“¦ æŒ‰é¡µæ•°åˆ†æ‰¹ (æ¯æ‰¹æœ€å¤š {batch_size} é¡µ):")

    for i, pdf_file in enumerate(pdf_files):
        page_count = get_pdf_page_count(pdf_file)

        if page_count > max_pages_per_pdf:
            continue
        pdf_file_name = os.path.basename(pdf_file).replace(".pdf", "")
        # #åˆ†é¡µæ—¶æå‰è®°å½•ç»“æœæ–‡ä»¶é¡µæ•°
        #
        # target_file = f"{output_path}/{pdf_file_name}.json.zip"
        # page_info = {
        #     'input_path': pdf_file,
        #     'output_path': target_file,
        #     'page_count': page_count,
        # }
        # page_result_path = f"{output_path}/page_result"
        # if not os.path.exists(page_result_path):
        #     os.makedirs(page_result_path, exist_ok=True)
        # json_file_name = f"{pdf_file_name}.json"
        # temp_json_path = os.path.join(page_result_path, json_file_name)
        # with open(temp_json_path, 'w') as f:
        #     json.dump(page_info, f)

        # å¦‚æœå•ä¸ªæ–‡ä»¶å°±è¶…è¿‡æ‰¹æ¬¡å¤§å°ï¼Œå•ç‹¬ä½œä¸ºä¸€æ‰¹
        if page_count >= batch_size:
            if current_batch:  # å…ˆå¤„ç†å½“å‰æ‰¹æ¬¡
                batches.append({
                    'files': current_batch.copy(),
                    'total_pages': current_batch_pages,
                    'file_names': [os.path.basename(f) for f in current_batch]
                })
                logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")
                current_batch = []
                current_batch_pages = 0

            # å¤§æ–‡ä»¶å•ç‹¬ä¸€æ‰¹
            batches.append({
                'files': [pdf_file],
                'total_pages': page_count,
                'file_names': [pdf_file_name]
            })
            logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {pdf_file}, {page_count} é¡µ (å¤§æ–‡ä»¶å•ç‹¬æ‰¹æ¬¡)")
            continue

        # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šè¿™ä¸ªæ–‡ä»¶ä¼šè¶…è¿‡é™åˆ¶ï¼Œå…ˆå¤„ç†å½“å‰æ‰¹æ¬¡
        if current_batch_pages + page_count > batch_size:
            batches.append({
                'files': current_batch.copy(),
                'total_pages': current_batch_pages,
                'file_names': [os.path.basename(f) for f in current_batch]
            })
            logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")
            current_batch = []
            current_batch_pages = 0

        # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
        current_batch.append(pdf_file)
        current_batch_pages += page_count

    # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if current_batch:
        batches.append({
            'files': current_batch,
            'total_pages': current_batch_pages,
            'file_names': [os.path.basename(f) for f in current_batch]
        })
        logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")

    return batches


class SimpleMinerUPool:
    """ä¿®å¤çš„MinerUå¤„ç†æ±  - ç®€åŒ–ç‰ˆæœ¬"""

    def __init__(self, gpu_ids: List[int], workers_per_gpu: int = 2,
                 vram_size_gb: int = 24, max_pages_per_pdf: Optional[int] = None,
                 batch_size: Optional[int] = None):
        self.gpu_ids = gpu_ids
        self.workers_per_gpu = workers_per_gpu
        self.vram_size_gb = vram_size_gb
        self.max_pages_per_pdf = max_pages_per_pdf
        self.batch_size = batch_size

        # è®¾ç½®ç¯å¢ƒå˜é‡ - å¢åŠ å†…å­˜ä½¿ç”¨é…ç½®
        os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram_size_gb)
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

        # è®¾ç½®batchç›¸å…³ç¯å¢ƒå˜é‡
        if batch_size is not None:
            os.environ['MINERU_MIN_BATCH_INFERENCE_SIZE'] = str(batch_size)
            logging.info(f"Set batch size to: {batch_size}")

        # åˆ›å»ºåŸºäºGPU IDçš„è¿›ç¨‹æ± 
        self.process_pool = SimpleProcessPool(gpu_ids=gpu_ids, workers_per_gpu=workers_per_gpu)
        logging.info(
            f"Created MinerU pool: {len(gpu_ids)} GPUs Ã— {workers_per_gpu} workers = {len(gpu_ids) * workers_per_gpu} total workers")

    def process_pdf_files(self, pdf_files: List[str], output_dir: str) -> List[Dict]:
        """å¤„ç†PDFæ–‡ä»¶åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
        logging.info(f"Processing {len(pdf_files)} PDF files using {len(self.gpu_ids)} GPUs...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        files_to_process = []
        for pdf_path in pdf_files:
            pdf_name = os.path.basename(pdf_path).replace(".pdf", "")
            target_file = f"{output_dir}/result/{pdf_name}.json.zip"
            if os.path.exists(target_file):
                logging.info(f"Already processed: {pdf_path} -> {target_file}")
                continue
            files_to_process.append(pdf_path)

        if not files_to_process:
            logging.warning("No files need processing")
            return []

        logging.info(f"After filtering: {len(files_to_process)} files to process")

        results = []
        task_info = {}  # å­˜å‚¨ä»»åŠ¡IDå’Œè¾“å…¥è·¯å¾„çš„æ˜ å°„

        try:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            batch_size = int(os.environ.get('DEFAULT_BATCH_SIZE', '384'))
            start = time.time()
            batches = create_batches_by_pages(files_to_process, batch_size, output_dir)
            logging.info(f"åˆ†æ‰¹è€—æ—¶ï¼š{time.time() - start}")
            for batch in batches:
                task_data = (batch, output_dir)
                task_id = self.process_pool.submit_task(gpu_worker_task, *task_data)
                task_info[task_id] = batch

            logging.info(f"Submitted {len(batches)} tasks to process pool")

            # è®¾ç½®å®Œæˆä¿¡å·
            self.process_pool.set_complete_signal()

            # æ”¶é›†ç»“æœ
            start_time = time.time()

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for _ in range(len(batches)):
                result = self.process_pool.get_result()
                if result:
                    task_id, status, data = result
                    pdf_path = task_info.get(task_id, "unknown")

                    if status == 'success':
                        results.append(data)
                        logging.info(f"Task completed: {pdf_path}")
                    elif status == 'error':
                        error_result = {
                            'success': False,
                            'error': data,
                            'input_path': pdf_path
                        }
                        results.append(error_result)
                        logging.error(f"Task failed: {pdf_path} with error: {data}")

            total_time = time.time() - start_time
            success_count = sum(1 for r in results if r.get('success', False))
            skipped_count = sum(1 for r in results if r.get('skipped', False))

            logging.info(f"\nProcessing complete!")
            logging.info(f"Total time: {total_time:.1f} seconds")
            logging.info(
                f"Success: {success_count}, Skipped: {skipped_count}, Errors: {len(results) - success_count - skipped_count}")

            if success_count > 0:
                logging.info(f"Average: {total_time / success_count:.2f} seconds per successful file")

            return results

        except Exception as e:
            logging.error(f"Unexpected error in process_pdf_files: {e}")
            traceback.print_exc()
            return results
        finally:
            logging.info("Shutting down process pool...")
            self.process_pool.shutdown()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # ç¡®ä¿è¿›ç¨‹æ± è¢«æ­£ç¡®å…³é—­
        if hasattr(self, 'process_pool'):
            self.process_pool.shutdown()


def process_pdfs(input_dir, output_dir, gpu_ids='0,1,2,3,4,5,6,7', workers_per_gpu=2,
                 vram_size_gb=24, max_pages=None, shuffle=False,
                 batch_size=None):
    """å¤„ç†PDFæ–‡ä»¶çš„å‡½æ•°ï¼Œå¯é€šè¿‡å‚æ•°ç›´æ¥è°ƒç”¨"""
    # è§£æGPU ID
    gpu_ids = [int(x.strip()) for x in gpu_ids.split(',')]

    # è·å–PDFæ–‡ä»¶
    pdf_files = glob.glob(f"{input_dir}/*.pdf")
    logging.info(f"Found {len(pdf_files)} PDF files")
    logging.info(f"Using GPUs: {gpu_ids}")
    logging.info(f"Workers per GPU: {workers_per_gpu}")
    logging.info(f"Max pages per PDF: {max_pages or 'No limit'}")

    if not pdf_files:
        logging.warning("No PDF files found!")
        return

    # åˆ›å»ºå¤„ç†æ± å¹¶è¿è¡Œ
    with SimpleMinerUPool(
            gpu_ids=gpu_ids,
            workers_per_gpu=workers_per_gpu,
            vram_size_gb=vram_size_gb,
            max_pages_per_pdf=max_pages,
            batch_size=batch_size
    ) as pool:
        results = pool.process_pdf_files(pdf_files, output_dir)

    return results

