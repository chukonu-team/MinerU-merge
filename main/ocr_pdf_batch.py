#!/usr/bin/env python3
"""ä¿®å¤çš„MinerUè¿›ç¨‹æ±  - ç®€åŒ–ç‰ˆæœ¬"""

import os
import time
import json
import glob
import random
from typing import List, Dict, Any, Optional
import copy
import zipfile
import traceback
import logging
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.cli.common import read_fn, convert_pdf_bytes_to_bytes_by_pypdfium2

# å¯¼å…¥ç®€åŒ–çš„è¿›ç¨‹æ± 
from process_pool import SimpleProcessPool

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [PID:%(process)d][%(thread)d] %(levelname)s: %(message)s"
)


def get_pdf_page_count(pdf_path):
    """ä½¿ç”¨fitzè·å–PDFé¡µæ•°"""
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        page_count = len(doc)
        doc.close()
        return page_count
    except Exception as e:
        logging.warning(f"Error getting page count for {pdf_path}: {e}")
        return 0


def preprocessing_worker(batch, save_dir, **kwargs):
    """
    å¢å¼ºç‰ˆé¢„å¤„ç†å‡½æ•° - è¯»å–PDFæ–‡ä»¶ã€è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼ã€åŠ è½½å›¾åƒ
    è¿™éƒ¨åˆ†å·¥ä½œåŒ…å«load_images_from_pdfï¼Œä¸ä¾èµ–GPUï¼Œå¯ä»¥åœ¨CPUä¸Šå¹¶è¡Œå¤„ç†
    """
    start = time.time()
    logging.info(f"å¢å¼ºé¢„å¤„ç†æ‰¹æ¬¡å¼€å§‹: {batch.get('file_names', [])}")

    pdf_bytes_list = []
    local_image_dirs = []
    pdf_paths = batch['files'].copy()

    read_start = time.time()

    # è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼
    for pdf_path in pdf_paths:
        try:
            pdf_bytes = read_fn(pdf_path)
            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, 0, None)
            pdf_bytes_list.append(pdf_bytes)
            pdf_name = os.path.basename(pdf_path)
            local_image_dir = f"/tmp/data/mineru_ocr_local_image_dir/{pdf_name}"
            if not os.path.exists(local_image_dir):
                os.makedirs(local_image_dir, exist_ok=True)
            local_image_dirs.append(local_image_dir)
        except Exception as e:
            logging.warning(f"åŠ è½½ {pdf_path} å¤±è´¥: {e}")
            traceback.print_exc()
            # ä½¿ç”¨Noneå¡«å……å ä½ï¼Œä¿æŒåˆ—è¡¨é•¿åº¦ä¸€è‡´
            pdf_bytes_list.append(None)
            pdf_name = os.path.basename(pdf_path)
            local_image_dir = f"/tmp/data/mineru_ocr_local_image_dir/{pdf_name}"
            if not os.path.exists(local_image_dir):
                os.makedirs(local_image_dir, exist_ok=True)
            local_image_dirs.append(local_image_dir)

    preprocess_time = time.time() - read_start
    logging.info(f"PDFè¯»å–å®Œæ¯•ï¼Œè€—æ—¶{preprocess_time:.2f}ç§’")

    # åŠ è½½å›¾åƒï¼ˆè¿™éƒ¨åˆ†åŸåœ¨batch_doc_analyzeä¸­ï¼‰
    image_loading_start = time.time()
    from mineru.backend.vlm.vlm_analyze import load_images_from_pdf
    from mineru.utils.enum_class import ImageType

    all_images_list = []
    images_count_per_pdf = []
    pdf_processing_status = []

    # éå†æ‰€æœ‰PDFæ–‡æ¡£ï¼ŒåŠ è½½å›¾åƒï¼ˆä½¿ç”¨BYTESæ ¼å¼ä»¥æ”¯æŒåºåˆ—åŒ–ï¼‰
    for pdf_bytes in pdf_bytes_list:
        if pdf_bytes is None:
            # PDFå­—èŠ‚æ•°æ®ä¸ºç©ºï¼Œç›´æ¥æ ‡è®°ä¸ºå¤±è´¥
            images_count_per_pdf.append(0)
            pdf_processing_status.append(False)
            continue

        try:
            images_list, pdf_doc = load_images_from_pdf(pdf_bytes, image_type=ImageType.BYTES)
            all_images_list.extend(images_list)
            images_count_per_pdf.append(len(images_list))
            pdf_processing_status.append(True)  # æ ‡è®°ä¸ºæˆåŠŸå¤„ç†
            # ç«‹å³å…³é—­pdf_docå¯¹è±¡ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
            if pdf_doc is not None:
                try:
                    pdf_doc.close()
                except:
                    pass
        except Exception as e:
            logging.warning(f"ä»PDFåŠ è½½å›¾åƒå¤±è´¥: {e}")
            images_count_per_pdf.append(0)  # å›¾åƒæ•°é‡ä¸º0
            pdf_processing_status.append(False)  # æ ‡è®°ä¸ºå¤„ç†å¤±è´¥

    image_loading_time = time.time() - image_loading_start
    logging.info(f"å›¾åƒåŠ è½½å®Œæ¯•ï¼Œè€—æ—¶{image_loading_time:.2f}ç§’")

    # ç›´æ¥ä½¿ç”¨pdf_bytesï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
    logging.info(f"ä½¿ç”¨pdf_bytesæ–¹å¼ï¼Œé¿å…pdf_docåºåˆ—åŒ–é—®é¢˜")



    total_preprocess_time = time.time() - start
    logging.info(f"å¢å¼ºé¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶{total_preprocess_time:.2f}ç§’ï¼Œæœ‰æ•ˆå›¾åƒæ•°: {len(all_images_list)}")

    # è¿‡æ»¤æ‰æ— æ•ˆçš„PDFï¼Œåªä¿ç•™æˆåŠŸçš„
    valid_indices = [i for i, status in enumerate(pdf_processing_status) if status]
    valid_pdf_bytes_list = [pdf_bytes_list[i] for i in valid_indices]
    valid_local_image_dirs = [local_image_dirs[i] for i in valid_indices]
    valid_pdf_paths = [pdf_paths[i] for i in valid_indices]
    valid_images_count_per_pdf = [images_count_per_pdf[i] for i in valid_indices]
    valid_pdf_processing_status = [pdf_processing_status[i] for i in valid_indices]

    # é‡æ–°æ„å»ºæœ‰æ•ˆçš„all_images_listï¼ŒåªåŒ…å«æˆåŠŸPDFçš„å›¾åƒ
    valid_all_images_list = []
    image_idx = 0
    for i, count in enumerate(images_count_per_pdf):
        if pdf_processing_status[i] and count > 0:
            # æ·»åŠ å½“å‰PDFçš„æ‰€æœ‰å›¾åƒåˆ°æœ‰æ•ˆåˆ—è¡¨
            valid_all_images_list.extend(all_images_list[image_idx:image_idx + count])
        image_idx += count

    successful_count = len(valid_indices)
    total_count = len(pdf_paths)

    logging.info(f"è¿‡æ»¤ç»“æœ: æˆåŠŸ {successful_count}/{total_count} ä¸ªPDFï¼Œæœ‰æ•ˆå›¾åƒæ•°: {len(valid_all_images_list)}")
    if successful_count < total_count:
        failed_indices = [i for i, status in enumerate(pdf_processing_status) if not status]
        failed_files = [os.path.basename(pdf_paths[i]) for i in failed_indices]
        logging.warning(f"è·³è¿‡çš„æ–‡ä»¶: {failed_files}")

    # è¿”å›è¿‡æ»¤åçš„é¢„å¤„ç†æ•°æ®ï¼ˆåªåŒ…å«æˆåŠŸçš„PDFï¼‰
    return {
        'pdf_bytes_list': valid_pdf_bytes_list,
        'local_image_dirs': valid_local_image_dirs,
        'pdf_paths': valid_pdf_paths,
        'save_dir': save_dir,
        'preprocess_time': preprocess_time,
        'image_loading_time': image_loading_time,
        'total_preprocess_time': total_preprocess_time,
        'all_images_list': valid_all_images_list,  # åªåŒ…å«æˆåŠŸPDFçš„byteså›¾åƒæ•°æ®
        'images_count_per_pdf': valid_images_count_per_pdf,
        'pdf_processing_status': valid_pdf_processing_status,  # ç°åœ¨å…¨éƒ¨ä¸ºTrue
        'images_pil_list': [],  # æ¸…ç©ºPILå›¾åƒåˆ—è¡¨ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
        'batch_info': batch,
        'original_batch_size': total_count,  # ä¿å­˜åŸå§‹æ‰¹æ¬¡å¤§å°ç”¨äºæ—¥å¿—
        'successful_count': successful_count  # ä¿å­˜æˆåŠŸæ•°é‡
    }



def gpu_processing_task_with_preloaded_images(preprocessed_data, **kwargs):
    """
    GPUå¤„ç†å‡½æ•° - ä½¿ç”¨GPUè¿›è¡Œæ–‡æ¡£åˆ†æçš„ç¬¬ä¸€é˜¶æ®µï¼ˆåªåŒ…å«batch_two_step_extractï¼‰
    è¿™éƒ¨åˆ†å·¥ä½œéœ€è¦GPUï¼Œåªå¤„ç†æ¨ç†éƒ¨åˆ†ï¼Œç»“æœä¿å­˜äº¤ç»™åå¤„ç†CPUé˜Ÿåˆ—
    é¢„å¤„ç†é˜¶æ®µå·²ç»è¿‡æ»¤æ‰æ— æ•ˆPDFï¼Œè¿™é‡Œåªå¤„ç†æœ‰æ•ˆçš„æ•°æ®
    """
    start = time.time()
    logging.info(f"=== GPUå¤„ç†é˜¶æ®µ1è¯¦ç»†æ—¶é—´åˆ†æå¼€å§‹ ===")

    # ä»é¢„å¤„ç†æ•°æ®ä¸­è·å–æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼ˆå·²ç»æ˜¯è¿‡æ»¤åçš„æœ‰æ•ˆæ•°æ®ï¼‰
    all_images_list = preprocessed_data['all_images_list']
    pdf_bytes_list = preprocessed_data['pdf_bytes_list']  # PDFå­—èŠ‚æ•°æ®
    images_count_per_pdf = preprocessed_data['images_count_per_pdf']
    pdf_processing_status = preprocessed_data['pdf_processing_status']  # ç°åœ¨åº”è¯¥å…¨éƒ¨ä¸ºTrue
    save_dir = preprocessed_data['save_dir']
    local_image_dirs = preprocessed_data['local_image_dirs']
    batch = preprocessed_data['batch_info']

    original_batch_size = preprocessed_data.get('original_batch_size', 0)
    successful_count = preprocessed_data.get('successful_count', 0)

    logging.info(f"GPUå¤„ç†å¼€å§‹: åŸå§‹æ‰¹æ¬¡{original_batch_size}ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸ{successful_count}ä¸ªï¼Œæœ‰æ•ˆå›¾åƒæ•°: {len(all_images_list)}")

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
    if not all_images_list:
        logging.warning("æ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œè·³è¿‡GPUå¤„ç†")
        return {
            'success': True,
            'gpu_results': [],
            'preprocess_time': preprocessed_data.get('preprocess_time', 0),
            'image_loading_time': preprocessed_data.get('image_loading_time', 0),
            'total_preprocess_time': preprocessed_data.get('total_preprocess_time', 0),
            'total_time': time.time() - start,
            'data_preprocess_time': 0,
            'model_init_time': 0,
            'gpu_time': 0,
            'batch_info': batch,
            'all_images_list': all_images_list,
            'pdf_bytes_list': pdf_bytes_list,
            'images_count_per_pdf': images_count_per_pdf,
            'pdf_processing_status': pdf_processing_status,
            'save_dir': save_dir,
            'local_image_dirs': local_image_dirs
        }

    try:
        # æ­¥éª¤1: æ•°æ®é¢„å¤„ç† - ä»byteså›¾åƒæ•°æ®é‡å»ºPILå›¾åƒåˆ—è¡¨
        data_preprocess_start = time.time()
        images_pil_list = []

        # è½¬æ¢byteså›¾åƒæ•°æ®ä¸ºPILæ ¼å¼ï¼ˆæ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ‰æ•ˆçš„ï¼‰
        for image_dict in all_images_list:
            try:
                from mineru.utils.pdf_reader import bytes_to_pil
                pil_img = bytes_to_pil(image_dict["img_bytes"])
                images_pil_list.append(pil_img)
            except Exception as e:
                logging.warning(f"è½¬æ¢byteså›¾åƒåˆ°PILå¤±è´¥: {e}")

        data_preprocess_time = time.time() - data_preprocess_start
        logging.info(f"æ­¥éª¤1 - æ•°æ®é¢„å¤„ç† (bytesâ†’PILè½¬æ¢): {data_preprocess_time:.2f}ç§’ï¼Œè½¬æ¢å›¾åƒæ•°: {len(images_pil_list)}")

        # æ­¥éª¤2: æ¨¡å‹åˆå§‹åŒ–
        model_init_start = time.time()
        backend = os.environ.get("BACKEND", "vllm-engine")
        logging.info(f"backend: {backend}")

        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°ï¼Œåªä¼ é€’æœ‰æ•ˆçš„GPUé…ç½®
        filtered_kwargs = kwargs.copy()
        gpu_memory_utilization = os.environ.get("GPU_MEMORY_UTILIZATION",0.5)
        if 'gpu_id' in filtered_kwargs:
            del filtered_kwargs['gpu_id']  # vLLMä¸æ”¯æŒgpu_idå‚æ•°

        from mineru.backend.vlm.vlm_analyze import ModelSingleton

        predictor = ModelSingleton().get_model(backend, None, None, gpu_memory_utilization=gpu_memory_utilization, **filtered_kwargs)
        model_init_time = time.time() - model_init_start
        logging.info(f"æ­¥éª¤2 - æ¨¡å‹åˆå§‹åŒ–: {model_init_time:.2f}ç§’")

        # æ­¥éª¤3: GPUæ¨ç† - åªè°ƒç”¨batch_two_step_extract
        gpu_start = time.time()
        gpu_results = predictor.batch_two_step_extract(images=images_pil_list)
        gpu_time = time.time() - gpu_start
        logging.info(f"æ­¥éª¤3 - GPUæ¨ç† (çº¯AIæ¨¡å‹æ¨ç†): {gpu_time:.2f}ç§’ï¼Œæ¨ç†ç»“æœæ•°: {len(gpu_results)}")

        total_time = time.time() - start
        logging.info(f"GPUå¤„ç†é˜¶æ®µ1å®Œæˆï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’")

        # è¯¦ç»†æ—¶é—´åˆ†ææ±‡æ€»
        logging.info(f"=== GPUå¤„ç†é˜¶æ®µ1è¯¦ç»†æ—¶é—´åˆ†ææ±‡æ€» ===")
        logging.info(f"æ•°æ®é¢„å¤„ç† (bytesâ†’PILè½¬æ¢): {data_preprocess_time:.2f}ç§’ ({data_preprocess_time/total_time*100:.1f}%)")
        logging.info(f"æ¨¡å‹åˆå§‹åŒ–: {model_init_time:.2f}ç§’ ({model_init_time/total_time*100:.1f}%)")
        logging.info(f"GPUæ¨ç† (çº¯AIæ¨¡å‹æ¨ç†): {gpu_time:.2f}ç§’ ({gpu_time/total_time*100:.1f}%)")

        # éªŒè¯GPUç»“æœæ•°é‡ä¸å›¾åƒæ•°é‡åŒ¹é…
        if len(gpu_results) != len(images_pil_list):
            logging.warning(f"GPUç»“æœæ•°é‡({len(gpu_results)})ä¸å›¾åƒæ•°é‡({len(images_pil_list)})ä¸åŒ¹é…")

        # è¿”å›GPUå¤„ç†ç»“æœï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯ç»™åå¤„ç†é˜¶æ®µ
        return {
            'success': True,
            'gpu_results': gpu_results,
            'preprocess_time': preprocessed_data.get('preprocess_time', 0),
            'image_loading_time': preprocessed_data.get('image_loading_time', 0),
            'total_preprocess_time': preprocessed_data.get('total_preprocess_time', 0),
            'total_time': total_time,
            'data_preprocess_time': data_preprocess_time,
            'model_init_time': model_init_time,
            'gpu_time': gpu_time,
            'batch_info': batch,
            'all_images_list': all_images_list,
            'pdf_bytes_list': pdf_bytes_list,
            'images_count_per_pdf': images_count_per_pdf,
            'pdf_processing_status': pdf_processing_status,
            'save_dir': save_dir,
            'local_image_dirs': local_image_dirs
        }

    except Exception as e:
        error_time = time.time() - start
        logging.error(f"GPUå¤„ç†é˜¶æ®µ1å¤±è´¥: {e}")
        traceback.print_exc()
        # å³ä½¿GPUå¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦è¿”å›é”™è¯¯ä¿¡æ¯ç»™åå¤„ç†é˜¶æ®µå¤„ç†
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'error_time': error_time,
            'batch_info': batch,
            'all_images_list': all_images_list,
            'pdf_bytes_list': pdf_bytes_list,
            'images_count_per_pdf': images_count_per_pdf,
            'pdf_processing_status': pdf_processing_status,
            'save_dir': save_dir,
            'local_image_dirs': local_image_dirs
        }


def postprocessing_task(gpu_result_data, **kwargs):
    """
    åå¤„ç†CPUé˜Ÿåˆ—ä»»åŠ¡å‡½æ•° - å¤„ç†GPUç»“æœåçš„æ–‡ä»¶ä¿å­˜ç­‰è€—æ—¶æ“ä½œ
    è¿™éƒ¨åˆ†å·¥ä½œåœ¨CPUä¸Šå¤„ç†ï¼ŒåŒ…æ‹¬result_to_middle_jsonã€JSONåºåˆ—åŒ–ã€ZIPå‹ç¼©å†™å…¥ç­‰

    æ³¨æ„ï¼šç°åœ¨æ¥æ”¶åˆ°çš„æ˜¯è¿‡æ»¤åçš„æ•°æ®ï¼Œéœ€è¦ä¸ºæ‰€æœ‰åŸå§‹æ–‡ä»¶ç”Ÿæˆç»“æœï¼ˆåŒ…æ‹¬å¤±è´¥çš„ï¼‰
    """
    start = time.time()
    logging.info(f"=== åå¤„ç†CPUä»»åŠ¡å¼€å§‹ ===")

    # ä»GPUå¤„ç†ç»“æœä¸­è·å–æ‰€æœ‰å¿…è¦ä¿¡æ¯
    gpu_results = gpu_result_data.get('gpu_results', [])
    all_images_list = gpu_result_data.get('all_images_list', [])
    pdf_bytes_list = gpu_result_data.get('pdf_bytes_list', [])
    images_count_per_pdf = gpu_result_data.get('images_count_per_pdf', [])
    pdf_processing_status = gpu_result_data.get('pdf_processing_status', [])
    save_dir = gpu_result_data.get('save_dir')
    local_image_dirs = gpu_result_data.get('local_image_dirs', [])
    batch = gpu_result_data.get('batch_info', {})

    # è·å–åŸå§‹æ‰¹æ¬¡ä¿¡æ¯
    original_batch_size = gpu_result_data.get('original_batch_size', 0)
    successful_count = gpu_result_data.get('successful_count', 0)

    # æ£€æŸ¥GPUå¤„ç†æ˜¯å¦æˆåŠŸ
    gpu_success = gpu_result_data.get('success', False)
    gpu_error = gpu_result_data.get('error')

    logging.info(f"åå¤„ç†å¼€å§‹: åŸå§‹æ‰¹æ¬¡{original_batch_size}ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸ{successful_count}ä¸ªï¼ŒGPUæˆåŠŸ={gpu_success}")

    try:
        # æ­¥éª¤1: ç»“æœåå¤„ç† - ä¸ºæ¯ä¸ªPDFæ–‡æ¡£åˆ†åˆ«ç”Ÿæˆmiddle_json
        postprocess_start = time.time()
        all_middle_json = []
        image_idx = 0

        for i, is_success in enumerate(pdf_processing_status):
            logging.info(f"æ­¥éª¤1.1 loop index:{i} - é¢„å¤„ç†æˆåŠŸ: {is_success}")

            # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œç›´æ¥è¿”å›None
            if not is_success:
                all_middle_json.append(None)
                continue

            # å¦‚æœGPUå¤„ç†å¤±è´¥ï¼Œä¹Ÿè¿”å›None
            if not gpu_success:
                all_middle_json.append(None)
                continue

            # è·å–å½“å‰PDFçš„å›¾åƒæ•°é‡
            current_pdf_images_count = images_count_per_pdf[i]
            if current_pdf_images_count == 0:
                # å¯¹äºæ²¡æœ‰å›¾åƒçš„PDFï¼Œè¿”å›None
                all_middle_json.append(None)
                continue

            # è·å–å½“å‰PDFçš„å›¾åƒåˆ—è¡¨å’Œç»“æœ
            # current_images_list = all_images_list[image_idx: image_idx + current_pdf_images_count]
            current_images_list = []
            for image_dict in all_images_list[image_idx: image_idx + current_pdf_images_count]:
                if image_dict and isinstance(image_dict, dict) and "img_bytes" in image_dict:
                    # å°†bytesè½¬æ¢å›PILå›¾åƒ
                    try:
                        from mineru.utils.pdf_reader import bytes_to_pil
                        pil_img = bytes_to_pil(image_dict["img_bytes"])
                        current_images_list.append({"img_pil":pil_img,"scale":image_dict["scale"]})
                    except Exception as e:
                        logging.warning(f"è½¬æ¢byteså›¾åƒåˆ°PILå¤±è´¥: {e}")





            current_gpu_results = gpu_results[image_idx: image_idx + current_pdf_images_count]

            # ä¸ºå½“å‰PDFç”Ÿæˆmiddle_jsonï¼Œä½¿ç”¨pdf_bytesé¿å…åºåˆ—åŒ–é—®é¢˜
            if i >= len(pdf_bytes_list):
                logging.error(f"ç´¢å¼•è¶…å‡ºèŒƒå›´: i={i}, len(pdf_bytes_list)={len(pdf_bytes_list)}")
                all_middle_json.append(None)
                continue

            pdf_bytes = pdf_bytes_list[i]
            if pdf_bytes is None:
                logging.error(f"PDFå­—èŠ‚æ•°æ®ä¸ºç©º: i={i}")
                all_middle_json.append(None)
                continue

            try:
                from mineru.backend.vlm.vlm_analyze import result_to_middle_json
                import pypdfium2 as pdfium

                # ç›´æ¥ä»pdf_bytesåˆ›å»ºpdf_docå¯¹è±¡
                pdf_doc = pdfium.PdfDocument(pdf_bytes)
                local_image_dir = local_image_dirs[i]
                image_writer = FileBasedDataWriter(local_image_dir)
                try:
                    middle_json = result_to_middle_json(current_gpu_results, current_images_list, pdf_doc, image_writer)
                    all_middle_json.append(middle_json)
                    logging.info(f"æˆåŠŸç”Ÿæˆmiddle_json for PDF {i} from bytes")
                finally:
                    # ç¡®ä¿æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
                    try:
                        pdf_doc.close()
                    except:
                        pass

            except Exception as create_doc_error:
                logging.error(f"ä»bytesåˆ›å»ºPDFæ–‡æ¡£å¤±è´¥ {i}: {create_doc_error}")
                all_middle_json.append(None)

            # æ›´æ–°å›¾åƒç´¢å¼•
            image_idx += current_pdf_images_count

        postprocess_time = time.time() - postprocess_start
        logging.info(f"æ­¥éª¤1 - ç»“æœåå¤„ç† (è¡¨æ ¼åˆå¹¶ã€æ ‡é¢˜ä¼˜åŒ–ç­‰): {postprocess_time:.2f}ç§’")

        # æ­¥éª¤2: æ–‡ä»¶ä¿å­˜ - ä¿å­˜ç»“æœ
        file_save_start = time.time()
        final_results = []

        logging.info(f"=== æ­¥éª¤2 - æ–‡ä»¶ä¿å­˜è¯¦ç»†è°ƒè¯•å¼€å§‹ ===")
        logging.info(f"å¾…å¤„ç†æ–‡ä»¶æ•°é‡: {len(all_middle_json)}")
        logging.info(f"é¢„å¤„ç†çŠ¶æ€åˆ—è¡¨: {pdf_processing_status}")
        logging.info(f"batchä¿¡æ¯: {batch}")

        for i, (middle_json, is_success) in enumerate(zip(all_middle_json, pdf_processing_status)):
            logging.info(f"--- å¤„ç†æ–‡ä»¶ {i+1}/{len(all_middle_json)} ---")
            logging.info(f"  é¢„å¤„ç†æˆåŠŸ: {is_success}")
            logging.info(f"  middle_jsonæ˜¯å¦ä¸ºNone: {middle_json is None}")

            if not is_success:
                logging.warning(f"  è·³è¿‡æ–‡ä»¶ {i+1}: é¢„å¤„ç†å¤±è´¥")
                continue

            # ä»åŸå§‹batchä¿¡æ¯ä¸­è·å–PDFè·¯å¾„
            if 'files' in batch and i < len(batch['files']):
                pdf_path = batch['files'][i]
                logging.info(f"  PDFè·¯å¾„: {pdf_path}")
            else:
                logging.error(f"  æ— æ³•è·å–PDFè·¯å¾„: batch['files']å­˜åœ¨={('files' in batch)}, i={i}, len(batch['files'])={len(batch.get('files', []))}")
                continue

            pdf_file_name = os.path.basename(pdf_path).replace(".pdf", "")
            logging.info(f"  PDFæ–‡ä»¶å: {pdf_file_name}")

            if middle_json is not None:
                logging.info(f"  å¼€å§‹ç”Ÿæˆtarget_file...")

                try:
                    # æ­¥éª¤2.1: åˆ›å»ºæ¨ç†ç»“æœ
                    infer_result = {"middle_json": middle_json}
                    logging.info(f"  æ­¥éª¤2.1 - åˆ›å»ºæ¨ç†ç»“æœ: æˆåŠŸ")

                    # æ­¥éª¤2.2: JSONåºåˆ—åŒ–
                    res_json_str = json.dumps(infer_result, ensure_ascii=False)
                    json_size = len(res_json_str.encode('utf-8'))
                    logging.info(f"  æ­¥éª¤2.2 - JSONåºåˆ—åŒ–: æˆåŠŸ, å¤§å°: {json_size} bytes")

                    # æ­¥éª¤2.3: åˆ›å»ºç»“æœç›®å½•
                    result_dir = f"{save_dir}/result"
                    if not os.path.exists(result_dir):
                        os.makedirs(result_dir, exist_ok=True)
                        logging.info(f"  æ­¥éª¤2.3 - åˆ›å»ºç»“æœç›®å½•: {result_dir}")
                    else:
                        logging.info(f"  æ­¥éª¤2.3 - ç»“æœç›®å½•å·²å­˜åœ¨: {result_dir}")

                    # æ­¥éª¤2.4: æ„å»ºtarget_fileè·¯å¾„
                    target_file = f"{result_dir}/{pdf_file_name}.json.zip"
                    logging.info(f"  æ­¥éª¤2.4 - target_fileè·¯å¾„: {target_file}")

                    # æ­¥éª¤2.5: ZIPå‹ç¼©å†™å…¥
                    logging.info(f"  æ­¥éª¤2.5 - å¼€å§‹ZIPå‹ç¼©å†™å…¥...")
                    with zipfile.ZipFile(target_file, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                        res_json_bytes = res_json_str.encode("utf-8")
                        zf.writestr(f"{pdf_file_name}.json", res_json_bytes)
                    logging.info(f"  æ­¥éª¤2.5 - ZIPå‹ç¼©å†™å…¥å®Œæˆ")

                    # æ­¥éª¤2.6: è·å–æ–‡ä»¶ä¿¡æ¯
                    page_count = get_pdf_page_count(pdf_path)
                    file_size = os.path.getsize(target_file)
                    logging.info(f"  æ­¥éª¤2.6 - æ–‡ä»¶ä¿¡æ¯: é¡µæ•°={page_count}, æ–‡ä»¶å¤§å°={file_size} bytes")

                    # æ­¥éª¤2.7: åˆ›å»ºæˆåŠŸç»“æœ
                    result = {
                        'input_path': pdf_path,
                        'output_path': target_file,
                        'page_count': page_count,
                        'file_size': file_size,
                        'success': True
                    }
                    logging.info(f"  æ­¥éª¤2.7 - åˆ›å»ºæˆåŠŸç»“æœ: {target_file}")

                except Exception as save_error:
                    logging.error(f"  âŒ æ–‡ä»¶ä¿å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {save_error}")
                    logging.error(f"  è¯¦ç»†é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")

                    # æ­¥éª¤2.8: åˆ›å»ºå¤±è´¥ç»“æœ
                    result = {
                        'input_path': pdf_path,
                        'output_path': None,
                        'success': False,
                        'save_error': str(save_error)
                    }
                    logging.info(f"  æ­¥éª¤2.8 - åˆ›å»ºå¤±è´¥ç»“æœ")
            else:
                # GPUå¤„ç†å¤±è´¥æˆ–middle_jsonä¸ºNoneçš„æƒ…å†µ
                failure_reason = 'middle_json is None'
                if not gpu_success:
                    failure_reason = f'GPU processing failed: {gpu_error}'

                logging.warning(f"  æ— æ³•ä¿å­˜æ–‡ä»¶: {failure_reason}")
                result = {
                    'input_path': pdf_path,
                    'output_path': None,
                    'success': False,
                    'reason': failure_reason
                }

            # æ­¥éª¤2.9: ä¿å­˜é¡µé¢ç»“æœä¿¡æ¯
            page_result_path = f"{save_dir}/page_result"
            if not os.path.exists(page_result_path):
                os.makedirs(page_result_path, exist_ok=True)
                logging.info(f"  æ­¥éª¤2.9 - åˆ›å»ºé¡µé¢ç»“æœç›®å½•: {page_result_path}")

            json_file_name = f"{pdf_file_name}.json"
            temp_json_path = os.path.join(page_result_path, json_file_name)

            try:
                with open(temp_json_path, 'w') as f:
                    json.dump(result, f, indent=2)
                logging.info(f"  æ­¥éª¤2.9 - ä¿å­˜é¡µé¢ç»“æœæ–‡ä»¶: {temp_json_path}")
            except Exception as page_save_error:
                logging.error(f"  âŒ ä¿å­˜é¡µé¢ç»“æœæ–‡ä»¶å¤±è´¥: {page_save_error}")

            final_results.append(result)
            logging.info(f"  æ–‡ä»¶ {i+1} å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {result['success']}")

        logging.info(f"=== æ­¥éª¤2 - æ–‡ä»¶ä¿å­˜è¯¦ç»†è°ƒè¯•ç»“æŸ ===")
        success_count = sum(1 for r in final_results if r['success'])
        logging.info(f"æ–‡ä»¶ä¿å­˜ç»Ÿè®¡: æˆåŠŸ {success_count}/{len(final_results)} ä¸ªæ–‡ä»¶")

        file_save_time = time.time() - file_save_start
        logging.info(f"æ­¥éª¤2 - æ–‡ä»¶ä¿å­˜ (JSONåºåˆ—åŒ–ã€ZIPå‹ç¼©å†™å…¥): {file_save_time:.2f}ç§’")

        total_time = time.time() - start
        logging.info(f"åå¤„ç†CPUä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’")

        # è¯¦ç»†æ—¶é—´åˆ†ææ±‡æ€»
        logging.info(f"=== åå¤„ç†CPUä»»åŠ¡è¯¦ç»†æ—¶é—´åˆ†ææ±‡æ€» ===")
        if 'postprocess_time' in locals():
            logging.info(f"ç»“æœåå¤„ç† (è¡¨æ ¼åˆå¹¶ã€æ ‡é¢˜ä¼˜åŒ–ç­‰): {postprocess_time:.2f}ç§’ ({postprocess_time/total_time*100:.1f}%)")
        if 'file_save_time' in locals():
            logging.info(f"æ–‡ä»¶ä¿å­˜ (JSONåºåˆ—åŒ–ã€ZIPå‹ç¼©å†™å…¥): {file_save_time:.2f}ç§’ ({file_save_time/total_time*100:.1f}%)")

        # è¿”å›æœ€ç»ˆç»“æœ
        return {
            'success': True,
            'results': final_results,
            'preprocess_time': gpu_result_data.get('preprocess_time', 0),
            'image_loading_time': gpu_result_data.get('image_loading_time', 0),
            'gpu_time': gpu_result_data.get('gpu_time', 0),
            'total_preprocess_time': gpu_result_data.get('total_preprocess_time', 0),
            'total_time': total_time,
            'postprocess_time': postprocess_time if 'postprocess_time' in locals() else 0,
            'file_save_time': file_save_time if 'file_save_time' in locals() else 0,
            'batch_info': batch
        }

    except Exception as e:
        error_time = time.time() - start
        logging.error(f"åå¤„ç†CPUä»»åŠ¡å¤±è´¥: {e}")
        traceback.print_exc()

        # å³ä½¿åå¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦ä¸ºæ¯ä¸ªPDFåˆ›å»ºå¤±è´¥ç»“æœ
        error_results = []
        for i, is_success in enumerate(pdf_processing_status):
            if not is_success:
                continue

            if 'files' in batch and i < len(batch['files']):
                pdf_path = batch['files'][i]
                failure_reason = f'Postprocessing failed: {str(e)}'
                if not gpu_success:
                    failure_reason = f'GPU processing failed: {gpu_error}'

                error_results.append({
                    'input_path': pdf_path,
                    'output_path': None,
                    'success': False,
                    'reason': failure_reason
                })

                # ä¿å­˜é¡µé¢ç»“æœä¿¡æ¯
                pdf_file_name = os.path.basename(pdf_path).replace(".pdf", "")
                page_result_path = f"{save_dir}/page_result"
                if not os.path.exists(page_result_path):
                    os.makedirs(page_result_path, exist_ok=True)

                json_file_name = f"{pdf_file_name}.json"
                temp_json_path = os.path.join(page_result_path, json_file_name)
                try:
                    with open(temp_json_path, 'w') as f:
                        json.dump(error_results[-1], f, indent=2)
                except:
                    pass

        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'error_time': error_time,
            'results': error_results,
            'batch_info': batch
        }



def gpu_worker_task(batch, save_dir, **kwargs):
    """
    GPUå·¥ä½œè¿›ç¨‹çš„ä»»åŠ¡å‡½æ•° - é€‚é…ä¸‰çº§é˜Ÿåˆ—ç³»ç»Ÿ
    ç°åœ¨è¿™ä¸ªå‡½æ•°è°ƒç”¨å¢å¼ºé¢„å¤„ç†ä»»åŠ¡ï¼ˆåŒ…å«å›¾åƒåŠ è½½ï¼‰ï¼Œé¢„å¤„ç†ç»“æœä¼šè¢«æ”¾å…¥GPUé˜Ÿåˆ—
    """

    logging.info(f"GPU worker task å¼€å§‹å¢å¼ºé¢„å¤„ç†: {batch.get('file_names', [])}")
    try:
        # æ‰§è¡Œå¢å¼ºé¢„å¤„ç†ä»»åŠ¡ï¼ˆåŒ…å«å›¾åƒåŠ è½½ï¼‰
        preprocessed_data = preprocessing_worker(batch, save_dir, **kwargs)
        logging.info(f"å¢å¼ºé¢„å¤„ç†å®Œæˆï¼Œå‡†å¤‡æäº¤åˆ°GPUé˜Ÿåˆ—")
        return preprocessed_data
    except Exception as e:
        logging.error(f"å¢å¼ºé¢„å¤„ç†å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'batch_info': batch
        }


def create_batches_by_pages(pdf_files, batch_size, output_path, max_pages_per_pdf=1000):
    """
    æ ¹æ®é¡µæ•°åˆ›å»ºæ‰¹æ¬¡
    :param pdf_files: PDFæ–‡ä»¶åˆ—è¡¨
    :param batch_size: æ¯æ‰¹æ¬¡æœ€å¤§é¡µæ•°
    :return: æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«æ–‡ä»¶åˆ—è¡¨å’Œæ€»é¡µæ•°
    """
    batches = []
    current_batch = []
    current_batch_pages = 0

    logging.info(f"ğŸ“¦ æŒ‰é¡µæ•°åˆ†æ‰¹ (æ¯æ‰¹æœ€å¤š {batch_size} é¡µ):")

    for i, pdf_file in enumerate(pdf_files):
        page_count = get_pdf_page_count(pdf_file)

        if page_count > max_pages_per_pdf:
            continue
        pdf_file_name = os.path.basename(pdf_file).replace(".pdf", "")
        # #åˆ†é¡µæ—¶æå‰è®°å½•ç»“æœæ–‡ä»¶é¡µæ•°
        #
        # target_file = f"{output_path}/{pdf_file_name}.json.zip"
        # page_info = {
        #     'input_path': pdf_file,
        #     'output_path': target_file,
        #     'page_count': page_count,
        # }
        # page_result_path = f"{output_path}/page_result"
        # if not os.path.exists(page_result_path):
        #     os.makedirs(page_result_path, exist_ok=True)
        # json_file_name = f"{pdf_file_name}.json"
        # temp_json_path = os.path.join(page_result_path, json_file_name)
        # with open(temp_json_path, 'w') as f:
        #     json.dump(page_info, f)

        # å¦‚æœå•ä¸ªæ–‡ä»¶å°±è¶…è¿‡æ‰¹æ¬¡å¤§å°ï¼Œå•ç‹¬ä½œä¸ºä¸€æ‰¹
        if page_count >= batch_size:
            if current_batch:  # å…ˆå¤„ç†å½“å‰æ‰¹æ¬¡
                batches.append({
                    'files': current_batch.copy(),
                    'total_pages': current_batch_pages,
                    'file_names': [os.path.basename(f) for f in current_batch]
                })
                logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")
                current_batch = []
                current_batch_pages = 0

            # å¤§æ–‡ä»¶å•ç‹¬ä¸€æ‰¹
            batches.append({
                'files': [pdf_file],
                'total_pages': page_count,
                'file_names': [pdf_file_name]
            })
            logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {pdf_file}, {page_count} é¡µ (å¤§æ–‡ä»¶å•ç‹¬æ‰¹æ¬¡)")
            continue

        # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šè¿™ä¸ªæ–‡ä»¶ä¼šè¶…è¿‡é™åˆ¶ï¼Œå…ˆå¤„ç†å½“å‰æ‰¹æ¬¡
        if current_batch_pages + page_count > batch_size:
            batches.append({
                'files': current_batch.copy(),
                'total_pages': current_batch_pages,
                'file_names': [os.path.basename(f) for f in current_batch]
            })
            logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")
            current_batch = []
            current_batch_pages = 0

        # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
        current_batch.append(pdf_file)
        current_batch_pages += page_count

    # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if current_batch:
        batches.append({
            'files': current_batch,
            'total_pages': current_batch_pages,
            'file_names': [os.path.basename(f) for f in current_batch]
        })
        logging.info(f"  æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªæ–‡ä»¶, {current_batch_pages} é¡µ")

    return batches


class SimpleMinerUPool:
    """ä¿®å¤çš„MinerUå¤„ç†æ±  - ç®€åŒ–ç‰ˆæœ¬"""

    def __init__(self, gpu_ids: List[int], workers_per_gpu: int = 2,
                 vram_size_gb: int = 24, max_pages_per_pdf: Optional[int] = None,
                 batch_size: Optional[int] = None):
        self.gpu_ids = gpu_ids
        self.workers_per_gpu = workers_per_gpu
        self.vram_size_gb = vram_size_gb
        self.max_pages_per_pdf = max_pages_per_pdf
        self.batch_size = batch_size

        # è®¾ç½®ç¯å¢ƒå˜é‡ - å¢åŠ å†…å­˜ä½¿ç”¨é…ç½®
        os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram_size_gb)
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

        # è®¾ç½®batchç›¸å…³ç¯å¢ƒå˜é‡
        if batch_size is not None:
            os.environ['MINERU_MIN_BATCH_INFERENCE_SIZE'] = str(batch_size)
            logging.info(f"Set batch size to: {batch_size}")

        # åˆ›å»ºåŸºäºGPU IDçš„è¿›ç¨‹æ±  - ç°åœ¨æ”¯æŒä¸‰çº§é˜Ÿåˆ—
        self.process_pool = SimpleProcessPool(gpu_ids=gpu_ids, workers_per_gpu=workers_per_gpu,
            enable_preprocessing=True,
            max_gpu_queue_size=8,
            preprocessing_workers=4,
            postprocessing_workers=2)  # æ·»åŠ åå¤„ç†å·¥ä½œè¿›ç¨‹æ•°
        logging.info(
            f"Created MinerU pool: {len(gpu_ids)} GPUs Ã— {workers_per_gpu} workers = {len(gpu_ids) * workers_per_gpu} total workers")

    def process_pdf_files(self, pdf_files: List[str], output_dir: str) -> List[Dict]:
        """å¤„ç†PDFæ–‡ä»¶åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
        logging.info(f"Processing {len(pdf_files)} PDF files using {len(self.gpu_ids)} GPUs...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        files_to_process = []
        for pdf_path in pdf_files:
            pdf_name = os.path.basename(pdf_path).replace(".pdf", "")
            target_file = f"{output_dir}/result/{pdf_name}.json.zip"
            if os.path.exists(target_file):
                logging.info(f"Already processed: {pdf_path} -> {target_file}")
                continue
            files_to_process.append(pdf_path)

        if not files_to_process:
            logging.warning("No files need processing")
            return []

        logging.info(f"After filtering: {len(files_to_process)} files to process")

        results = []
        task_info = {}  # å­˜å‚¨ä»»åŠ¡IDå’Œè¾“å…¥è·¯å¾„çš„æ˜ å°„

        try:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            batch_size = int(os.environ.get('DEFAULT_BATCH_SIZE', '384'))
            start = time.time()
            batches = create_batches_by_pages(files_to_process, batch_size, output_dir)
            logging.info(f"åˆ†æ‰¹è€—æ—¶ï¼š{time.time() - start}")
            for batch in batches:
                task_data = (batch, output_dir)
                task_id = self.process_pool.submit_task(gpu_worker_task, *task_data)
                task_info[task_id] = batch

            logging.info(f"Submitted {len(batches)} tasks to process pool")

            # æ”¶é›†ç»“æœ
            start_time = time.time()

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for _ in range(len(batches)):
                result = self.process_pool.get_result()
                if result:
                    task_id, status, data = result
                    pdf_path = task_info.get(task_id, "unknown")

                    if status == 'success':
                        results.append(data)
                        logging.info(f"Task completed: {pdf_path}")
                    elif status == 'error':
                        error_result = {
                            'success': False,
                            'error': data,
                            'input_path': pdf_path
                        }
                        results.append(error_result)
                        logging.error(f"Task failed: {pdf_path} with error: {data}")

            total_time = time.time() - start_time
            success_count = sum(1 for r in results if r.get('success', False))
            skipped_count = sum(1 for r in results if r.get('skipped', False))

            logging.info(f"\nProcessing complete!")
            logging.info(f"Total time: {total_time:.1f} seconds")
            logging.info(
                f"Success: {success_count}, Skipped: {skipped_count}, Errors: {len(results) - success_count - skipped_count}")

            if success_count > 0:
                logging.info(f"Average: {total_time / success_count:.2f} seconds per successful file")

            return results

        except Exception as e:
            logging.error(f"Unexpected error in process_pdf_files: {e}")
            traceback.print_exc()
            return results
        finally:
            logging.info("Shutting down process pool...")
            self.process_pool.shutdown()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # ç¡®ä¿è¿›ç¨‹æ± è¢«æ­£ç¡®å…³é—­
        if hasattr(self, 'process_pool'):
            self.process_pool.shutdown()


def process_pdfs(input_dir, output_dir, gpu_ids='0,1,2,3,4,5,6,7', workers_per_gpu=2,
                 vram_size_gb=24, max_pages=None, shuffle=False,
                 batch_size=None):
    """å¤„ç†PDFæ–‡ä»¶çš„å‡½æ•°ï¼Œå¯é€šè¿‡å‚æ•°ç›´æ¥è°ƒç”¨"""
    # è§£æGPU ID
    gpu_ids = [int(x.strip()) for x in gpu_ids.split(',')]

    # è·å–PDFæ–‡ä»¶
    pdf_files = glob.glob(f"{input_dir}/*.pdf")
    logging.info(f"Found {len(pdf_files)} PDF files")
    logging.info(f"Using GPUs: {gpu_ids}")
    logging.info(f"Workers per GPU: {workers_per_gpu}")
    logging.info(f"Max pages per PDF: {max_pages or 'No limit'}")

    if not pdf_files:
        logging.warning("No PDF files found!")
        return

    # åˆ›å»ºå¤„ç†æ± å¹¶è¿è¡Œ
    with SimpleMinerUPool(
            gpu_ids=gpu_ids,
            workers_per_gpu=workers_per_gpu,
            vram_size_gb=vram_size_gb,
            max_pages_per_pdf=max_pages,
            batch_size=batch_size
    ) as pool:
        results = pool.process_pdf_files(pdf_files, output_dir)

    return results

