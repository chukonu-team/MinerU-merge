"""
åˆ†ç¦»Qwen2-VLæ¨¡å‹æµ‹è¯• (ä½¿ç”¨vLLMå®ç°ç¡®å®šæ€§è¾“å‡º)
ç¡®ä¿ä¸åŸå§‹æµ‹è¯•ä½¿ç”¨ç›¸åŒçš„æ¨ç†å¼•æ“å’Œå‚æ•°
"""

import torch
import vllm
import numpy as np
from PIL import Image
import json
import hashlib
from typing import List, Dict, Any
import os

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
def set_seeds(seed: int = 42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


class SeparatedQwen2VLTesterVLLM:
    """åˆ†ç¦»Qwen2-VLæ¨ç†æµ‹è¯•å™¨ - ä½¿ç”¨vLLMç¡®ä¿ä¸€è‡´æ€§"""

    def __init__(self, model_path: str, **vllm_kwargs):
        """
        åˆå§‹åŒ–åˆ†ç¦»æ¨¡å‹æµ‹è¯•å™¨ï¼ˆä½¿ç”¨vLLMï¼‰

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            **vllm_kwargs: vLLMå‚æ•°
        """
        print("ğŸ”§ åˆå§‹åŒ–åˆ†ç¦»Qwen2-VLæ¨¡å‹ (vLLM)...")

        self.model_path = model_path

        # è®¾ç½®ç§å­
        set_seeds(42)

        # ä½¿ç”¨ä¸åŸå§‹æµ‹è¯•å®Œå…¨ç›¸åŒçš„vLLMé…ç½®
        print("ğŸ“¦ åŠ è½½vLLMæ¨¡å‹...")
        self.llm = vllm.LLM(
            model=model_path,
            seed=42,
            **vllm_kwargs
        )

        self.tokenizer = self.llm.get_tokenizer()

        # è·å–processorç”¨äºå¤„ç†å›¾åƒ
        from transformers import Qwen2VLProcessor
        self.processor = Qwen2VLProcessor.from_pretrained(model_path)

        print("âœ… åˆ†ç¦»æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def load_test_images(self) -> List[Image.Image]:
        """åŠ è½½æµ‹è¯•å›¾åƒ"""
        images = []
        image_dir = "test_images"

        if not os.path.exists(image_dir):
            raise FileNotFoundError(f"æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")

        # åŠ è½½ç°æœ‰å›¾åƒ
        for i in range(1, 4):
            img_path = f"{image_dir}/test_image_{i}.png"
            if os.path.exists(img_path):
                img = Image.open(img_path)
                images.append(img)

        print(f"âœ… åŠ è½½äº† {len(images)} å¼ æµ‹è¯•å›¾åƒ")
        return images

    def get_test_prompts(self) -> List[str]:
        """è·å–æµ‹è¯•æç¤ºè¯"""
        prompts = [
            "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶å’Œæ–‡å­—ã€‚",
            "è¿™å¼ å›¾ç‰‡çš„ä¸»è¦é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå‡ ä½•å½¢çŠ¶ï¼Ÿ",
            "å¦‚æœè¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªlogoï¼Œå®ƒå¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
            "ç”¨ä¸€å¥è¯æ€»ç»“è¿™å¼ å›¾ç‰‡çš„ç‰¹å¾ã€‚",
        ]
        return prompts

    def run_inference_tests(self, images: List[Image.Image], prompts: List[str]) -> Dict[str, Any]:
        """è¿è¡Œåˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯•ï¼ˆä½¿ç”¨vLLMï¼‰"""
        print("ğŸ§ª è¿è¡Œåˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯• (vLLM)...")

        results = {
            "model_type": "separated_vllm",
            "model_info": {
                "seed": 42,
                "num_images": len(images),
                "num_prompts": len(prompts),
                "engine": "vllm",
                "architecture": "vit_encoder + llm_generator (vllm)"
            },
            "tests": []
        }

        # ä½¿ç”¨ä¸åŸå§‹æµ‹è¯•å®Œå…¨ç›¸åŒçš„é‡‡æ ·å‚æ•°ï¼ˆè´ªå©ªè§£ç ï¼‰
        sampling_params = vllm.SamplingParams(
            temperature=0.0,  # å®Œå…¨ç¡®å®šæ€§ï¼ˆè´ªå©ªè§£ç ï¼‰
            top_p=1.0,  # ä¸ä½¿ç”¨nucleus sampling
            max_tokens=150,
            seed=42,
            stop_token_ids=None
        )

        # å¯¹æ¯ä¸ªå›¾åƒå’Œæç¤ºè¯ç»„åˆè¿è¡Œæµ‹è¯•
        for img_idx, image in enumerate(images):
            print(f"\nå¤„ç†å›¾ç‰‡ {img_idx+1}/{len(images)}")

            for prompt_idx, prompt in enumerate(prompts):
                print(f"  æµ‹è¯•æç¤ºè¯ {prompt_idx+1}/{len(prompts)}: {prompt[:30]}...")

                try:
                    # ä½¿ç”¨processorå¤„ç†è¾“å…¥ï¼ˆä¸åŸå§‹æµ‹è¯•ç›¸åŒï¼‰
                    conversation = [
                        {
                            "role": "user",
                            "content": [
                                {"type": "image", "image": image},
                                {"type": "text", "text": prompt}
                            ]
                        }
                    ]

                    # åº”ç”¨å¯¹è¯æ¨¡æ¿
                    text = self.processor.apply_chat_template(
                        conversation,
                        tokenize=False,
                        add_generation_prompt=True
                    )

                    # ä½¿ç”¨vLLMç”Ÿæˆï¼ˆä¸åŸå§‹æµ‹è¯•å®Œå…¨ç›¸åŒçš„æ–¹å¼ï¼‰
                    from vllm.inputs import TextPrompt
                    prompt_input = TextPrompt(
                        prompt=text,
                        multi_modal_data={"image": image}
                    )

                    outputs = self.llm.generate(
                        prompt_input,
                        sampling_params=sampling_params
                    )

                    # æå–ç»“æœ
                    generated_text = outputs[0].outputs[0].text.strip()

                    # è®¡ç®—è¾“å‡ºå“ˆå¸Œç”¨äºå¯¹æ¯”
                    output_hash = hashlib.md5(generated_text.encode()).hexdigest()

                    test_result = {
                        "image_id": img_idx + 1,
                        "prompt_id": prompt_idx + 1,
                        "prompt": prompt,
                        "generated_text": generated_text,
                        "output_hash": output_hash,
                        "token_count": len(self.tokenizer.encode(generated_text)),
                        "finish_reason": outputs[0].outputs[0].finish_reason
                    }

                    results["tests"].append(test_result)
                    print(f"    âœ“ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)} å­—ç¬¦")

                except Exception as e:
                    print(f"    âŒ æ¨ç†å¤±è´¥: {str(e)}")
                    test_result = {
                        "image_id": img_idx + 1,
                        "prompt_id": prompt_idx + 1,
                        "prompt": prompt,
                        "generated_text": f"ERROR: {str(e)}",
                        "output_hash": "error",
                        "token_count": 0,
                        "finish_reason": "error"
                    }
                    results["tests"].append(test_result)

        print("\nâœ… åˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯•å®Œæˆ")
        return results

    def save_results(self, results: Dict[str, Any], filename: str = "separated_vllm_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹åˆ†ç¦»Qwen2-VLæ¨ç†æµ‹è¯• (vLLM)...")

        # åŠ è½½æµ‹è¯•æ•°æ®
        images = self.load_test_images()
        prompts = self.get_test_prompts()

        # è¿è¡Œæ¨ç†æµ‹è¯•
        results = self.run_inference_tests(images, prompts)

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        # æ˜¾ç¤ºæ‘˜è¦
        self.print_summary(results)

        return results

    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("åˆ†ç¦»æ¨¡å‹æµ‹è¯•æ‘˜è¦ (vLLM)")
        print("="*60)
        print(f"æ¨¡å‹ç±»å‹: {results['model_type']}")
        print(f"æ¨ç†å¼•æ“: {results['model_info']['engine']}")
        print(f"æ¶æ„: {results['model_info']['architecture']}")
        print(f"ç§å­: {results['model_info']['seed']}")
        print(f"æµ‹è¯•å›¾ç‰‡æ•°: {results['model_info']['num_images']}")
        print(f"æµ‹è¯•æç¤ºè¯æ•°: {results['model_info']['num_prompts']}")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(results['tests'])}")

        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        successful = [t for t in results['tests'] if t['output_hash'] != 'error']
        failed = [t for t in results['tests'] if t['output_hash'] == 'error']

        print(f"æˆåŠŸ: {len(successful)}, å¤±è´¥: {len(failed)}")

        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœç¤ºä¾‹
        print("\nç¤ºä¾‹ç»“æœ:")
        for i, test in enumerate(successful[:3]):
            print(f"\næµ‹è¯• {i+1}:")
            print(f"  å›¾ç‰‡ID: {test['image_id']}")
            print(f"  æç¤ºè¯: {test['prompt'][:50]}...")
            print(f"  ç”Ÿæˆé•¿åº¦: {len(test['generated_text'])} å­—ç¬¦")
            print(f"  è¾“å‡ºå“ˆå¸Œ: {test['output_hash'][:16]}...")
            print(f"  ç”Ÿæˆæ–‡æœ¬: {test['generated_text'][:80]}...")


def compare_with_original(original_file: str = "original_results.json",
                         separated_file: str = "separated_vllm_results.json"):
    """å¯¹æ¯”åŸå§‹å’Œåˆ†ç¦»æ¨¡å‹çš„ç»“æœ"""
    print("\n" + "="*80)
    print("ç»“æœä¸€è‡´æ€§å¯¹æ¯”åˆ†æ (vLLM vs vLLM)")
    print("="*80)

    # åŠ è½½ç»“æœ
    try:
        with open(original_file, 'r', encoding='utf-8') as f:
            original_results = json.load(f)
        print(f"âœ… å·²åŠ è½½åŸå§‹ç»“æœ: {original_file}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°åŸå§‹ç»“æœæ–‡ä»¶: {original_file}")
        return

    try:
        with open(separated_file, 'r', encoding='utf-8') as f:
            separated_results = json.load(f)
        print(f"âœ… å·²åŠ è½½åˆ†ç¦»ç»“æœ: {separated_file}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°åˆ†ç¦»ç»“æœæ–‡ä»¶: {separated_file}")
        return

    # å¯¹æ¯”åˆ†æ
    original_tests = {(t["image_id"], t["prompt_id"]): t for t in original_results["tests"]}
    separated_tests = {(t["image_id"], t["prompt_id"]): t for t in separated_results["tests"]}

    print(f"\nğŸ“Š å¯¹æ¯”ç»Ÿè®¡:")
    print(f"åŸå§‹æ¨¡å‹æµ‹è¯•æ•°: {len(original_tests)}")
    print(f"åˆ†ç¦»æ¨¡å‹æµ‹è¯•æ•°: {len(separated_tests)}")

    # è¯¦ç»†å¯¹æ¯”
    exact_matches = 0
    hash_matches = 0
    length_differences = []

    comparison_results = []

    for key in original_tests:
        if key not in separated_tests:
            print(f"âš ï¸ åˆ†ç¦»æ¨¡å‹ç¼ºå¤±æµ‹è¯•ç”¨ä¾‹: å›¾ç‰‡{key[0]}, æç¤ºè¯{key[1]}")
            continue

        orig = original_tests[key]
        sep = separated_tests[key]

        # è·³è¿‡é”™è¯¯çš„æµ‹è¯•
        if orig.get("output_hash") == "error" or sep.get("output_hash") == "error":
            continue

        # æ–‡æœ¬å®Œå…¨ä¸€è‡´
        text_match = orig["generated_text"] == sep["generated_text"]
        # å“ˆå¸Œä¸€è‡´
        hash_match = orig["output_hash"] == sep["output_hash"]
        # é•¿åº¦å·®å¼‚
        length_diff = abs(len(orig["generated_text"]) - len(sep["generated_text"]))

        if text_match:
            exact_matches += 1
        if hash_match:
            hash_matches += 1

        length_differences.append(length_diff)

        if not text_match:
            comparison_results.append({
                "image_id": key[0],
                "prompt_id": key[1],
                "prompt": orig["prompt"][:50] + "...",
                "text_match": text_match,
                "hash_match": hash_match,
                "length_diff": length_diff,
                "original_text": orig["generated_text"][:100] + "...",
                "separated_text": sep["generated_text"][:100] + "..."
            })

    # ç»Ÿè®¡ç»“æœ
    valid_tests = len(original_tests)
    if valid_tests > 0:
        exact_match_rate = (exact_matches / valid_tests) * 100
        hash_match_rate = (hash_matches / valid_tests) * 100
        avg_length_diff = sum(length_differences) / len(length_differences) if length_differences else 0

        print(f"\nğŸ“ˆ ä¸€è‡´æ€§åˆ†æç»“æœ:")
        print(f"æœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹: {valid_tests}")
        print(f"âœ… å®Œå…¨åŒ¹é…ç‡: {exact_match_rate:.2f}% ({exact_matches}/{valid_tests})")
        print(f"âœ… å“ˆå¸ŒåŒ¹é…ç‡: {hash_match_rate:.2f}% ({hash_matches}/{valid_tests})")
        print(f"ğŸ“ å¹³å‡é•¿åº¦å·®å¼‚: {avg_length_diff:.2f} å­—ç¬¦")

        # æ˜¾ç¤ºä¸åŒ¹é…çš„æƒ…å†µ
        if exact_matches < valid_tests:
            print(f"\nâš ï¸ å‘ç° {valid_tests - exact_matches} ä¸ªä¸åŒ¹é…çš„æ¡ˆä¾‹:")
            for i, comp in enumerate(comparison_results[:5], 1):
                print(f"\næ¡ˆä¾‹ {i} - å›¾{comp['image_id']}æç¤º{comp['prompt_id']}:")
                print(f"  æç¤ºè¯: {comp['prompt']}")
                print(f"  åŸå§‹: {comp['original_text']}")
                print(f"  åˆ†ç¦»: {comp['separated_text']}")
                print(f"  é•¿åº¦å·®å¼‚: {comp['length_diff']} å­—ç¬¦")

        # ç»“è®º
        if exact_match_rate == 100:
            print("\nğŸ‰ğŸ‰ğŸ‰ å®Œç¾ï¼åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†100%ä¸€è‡´ï¼")
        elif exact_match_rate > 95:
            print("\nğŸ‰ ä¼˜ç§€ï¼åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†é«˜åº¦ä¸€è‡´ï¼")
        elif exact_match_rate > 80:
            print("\nâœ… è‰¯å¥½ï¼åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†åŸºæœ¬ä¸€è‡´")
        else:
            print("\nâš ï¸ ä»å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""

    # é…ç½®æ¨¡å‹è·¯å¾„
    MODEL_PATH = "Qwen/Qwen2-VL-2B-Instruct"

    try:
        # åˆå§‹åŒ–åˆ†ç¦»æµ‹è¯•å™¨ï¼ˆä½¿ç”¨vLLMï¼‰
        tester = SeparatedQwen2VLTesterVLLM(
            model_path=MODEL_PATH,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            enforce_eager=True  # ä¸åŸå§‹æµ‹è¯•ç›¸åŒ
        )

        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = tester.run_full_test()

        print("\nğŸ‰ åˆ†ç¦»æ¨¡å‹æµ‹è¯•å®Œæˆï¼")

        # è‡ªåŠ¨è¿›è¡Œç»“æœå¯¹æ¯”
        print("\nğŸ” å¼€å§‹å¯¹æ¯”åŸå§‹å’Œåˆ†ç¦»æ¨¡å‹ç»“æœ...")
        compare_with_original()

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
