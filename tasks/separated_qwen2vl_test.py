"""
ä¿®å¤ç‰ˆæœ¬ï¼šåˆ†ç¦»Qwen2-VLæ¨¡å‹æµ‹è¯• (ViTåˆ†ç¦»æ¨ç†)
ä¸åŸå§‹æ¨¡å‹è¿›è¡Œä¸€è‡´æ€§å¯¹æ¯”éªŒè¯
"""

import torch
import numpy as np
from PIL import Image
import json
import hashlib
from typing import List, Dict, Any, Optional
import os
from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
def set_seeds(seed: int = 42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


class SeparatedQwen2VLTester:
    """åˆ†ç¦»Qwen2-VLæ¨ç†æµ‹è¯•å™¨ - ä½¿ç”¨transformersç›´æ¥æ“ä½œ"""

    def __init__(self, model_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–åˆ†ç¦»æ¨¡å‹æµ‹è¯•å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
        """
        print("ğŸ”§ åˆå§‹åŒ–åˆ†ç¦»Qwen2-VLæ¨¡å‹...")

        self.model_path = model_path
        self.device = device

        # è®¾ç½®ç§å­
        set_seeds(42)

        # åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆç”¨äºåˆ†ç¦»æ¨ç†ï¼‰
        print("ğŸ“¦ åŠ è½½å®Œæ•´Qwen2-VLæ¨¡å‹...")
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.model.eval()

        # åŠ è½½processor
        self.processor = Qwen2VLProcessor.from_pretrained(model_path)

        print("âœ… åˆ†ç¦»æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def load_test_images(self) -> List[Image.Image]:
        """åŠ è½½ä¸åŸå§‹æµ‹è¯•ç›¸åŒçš„å›¾åƒ"""
        images = []
        image_dir = "test_images"

        if not os.path.exists(image_dir):
            print("âš ï¸ æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡ŒåŸå§‹æµ‹è¯•ç”Ÿæˆå›¾åƒ")
            return self._create_same_test_images()

        # åŠ è½½ç°æœ‰å›¾åƒ
        for i in range(1, 4):  # åŠ è½½3å¼ æµ‹è¯•å›¾åƒ
            img_path = f"{image_dir}/test_image_{i}.png"
            if os.path.exists(img_path):
                img = Image.open(img_path)
                images.append(img)

        print(f"âœ… åŠ è½½äº† {len(images)} å¼ æµ‹è¯•å›¾åƒ")
        return images

    def _create_same_test_images(self) -> List[Image.Image]:
        """åˆ›å»ºä¸åŸå§‹æµ‹è¯•å®Œå…¨ç›¸åŒçš„æµ‹è¯•å›¾åƒ"""
        from PIL import ImageDraw, ImageFont

        images = []
        os.makedirs("test_images", exist_ok=True)

        # å›¾åƒ1: çº¢è‰²æ­£æ–¹å½¢
        img1 = Image.new('RGB', (336, 336), color='red')
        draw1 = ImageDraw.Draw(img1)
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        draw1.text((50, 150), "RED SQUARE", fill='white', font=font)
        img1.save("test_images/test_image_1.png")
        images.append(img1)

        # å›¾åƒ2: è“è‰²åœ†å½¢
        img2 = Image.new('RGB', (336, 336), color='lightblue')
        draw2 = ImageDraw.Draw(img2)
        draw2.ellipse([68, 68, 268, 268], fill='blue')
        draw2.text((120, 150), "BLUE", fill='white', font=font)
        img2.save("test_images/test_image_2.png")
        images.append(img2)

        # å›¾åƒ3: å½©è‰²æ¡çº¹
        img3 = Image.new('RGB', (336, 336), color='white')
        draw3 = ImageDraw.Draw(img3)
        colors = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']
        for i, color in enumerate(colors):
            y = i * 56
            draw3.rectangle([0, y, 336, y+56], fill=color)
        img3.save("test_images/test_image_3.png")
        images.append(img3)

        print(f"âœ… é‡æ–°åˆ›å»ºäº† {len(images)} å¼ æµ‹è¯•å›¾åƒ")
        return images

    def get_test_prompts(self) -> List[str]:
        """è·å–ä¸åŸå§‹æµ‹è¯•ç›¸åŒçš„æç¤ºè¯"""
        prompts = [
            "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶å’Œæ–‡å­—ã€‚",
            "è¿™å¼ å›¾ç‰‡çš„ä¸»è¦é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå‡ ä½•å½¢çŠ¶ï¼Ÿ",
            "å¦‚æœè¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªlogoï¼Œå®ƒå¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
            "ç”¨ä¸€å¥è¯æ€»ç»“è¿™å¼ å›¾ç‰‡çš„ç‰¹å¾ã€‚",
        ]
        return prompts

    def encode_image_separately(self, image: Image.Image) -> Dict[str, torch.Tensor]:
        """
        åˆ†ç¦»ç¼–ç ï¼šæå–å›¾åƒçš„è§†è§‰ç‰¹å¾åµŒå…¥

        Args:
            image: è¾“å…¥å›¾åƒ

        Returns:
            åŒ…å«å›¾åƒåµŒå…¥å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        print("  ğŸ” [åˆ†ç¦»æ­¥éª¤1] ä½¿ç”¨ViTç¼–ç å™¨æå–å›¾åƒç‰¹å¾...")

        # ä½¿ç”¨processorå¤„ç†å›¾åƒ
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": "dummy"}  # ä¸´æ—¶æ–‡æœ¬
                ]
            }
        ]

        # åº”ç”¨å¯¹è¯æ¨¡æ¿
        text = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=[text],
            images=[image],
            return_tensors="pt",
            padding=True
        )

        # ç§»åˆ°GPU
        inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                 for k, v in inputs.items()}

        # æå–å›¾åƒç‰¹å¾ï¼ˆé€šè¿‡å®Œæ•´çš„å‰å‘ä¼ æ’­ï¼Œä½†è®°å½•ä¸­é—´è¾“å‡ºï¼‰
        with torch.no_grad():
            pixel_values = inputs.get("pixel_values", None)
            image_grid_thw = inputs.get("image_grid_thw", None)

            if pixel_values is not None:
                # ä½¿ç”¨é’©å­æ•è·è§†è§‰ç‰¹å¾
                visual_features = None

                def hook_fn(module, input, output):
                    nonlocal visual_features
                    visual_features = output

                # æ³¨å†Œhookåˆ°è§†è§‰ç¼–ç å™¨çš„è¾“å‡º
                hook = self.model.visual.register_forward_hook(hook_fn)

                try:
                    # è¿è¡Œä¸€æ¬¡å®Œæ•´çš„å‰å‘ä¼ æ’­æ¥è·å–è§†è§‰ç‰¹å¾
                    _ = self.model(
                        input_ids=inputs["input_ids"],
                        attention_mask=inputs.get("attention_mask"),
                        pixel_values=pixel_values,
                        image_grid_thw=image_grid_thw,
                        output_hidden_states=True
                    )

                    if visual_features is not None:
                        print(f"    âœ“ å›¾åƒåµŒå…¥å½¢çŠ¶: {visual_features.shape}")
                    else:
                        print("    âš ï¸ æœªèƒ½æ•è·è§†è§‰ç‰¹å¾ï¼Œä½¿ç”¨å®Œæ•´æ¨ç†")

                finally:
                    hook.remove()

                return {
                    "visual_features": visual_features,
                    "pixel_values": pixel_values,
                    "image_grid_thw": image_grid_thw,
                    "input_ids": inputs["input_ids"],
                    "attention_mask": inputs.get("attention_mask", None)
                }
            else:
                raise ValueError("æœªæ‰¾åˆ°pixel_values")

    def generate_with_precomputed_embeds(
        self,
        prompt: str,
        image: Image.Image,
        precomputed_embeds: Optional[Dict[str, torch.Tensor]] = None,
        max_new_tokens: int = 150
    ) -> str:
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„å›¾åƒåµŒå…¥ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: æ–‡æœ¬æç¤º
            image: åŸå§‹å›¾åƒï¼ˆå¦‚æœéœ€è¦é‡æ–°å¤„ç†ï¼‰
            precomputed_embeds: é¢„è®¡ç®—çš„å›¾åƒåµŒå…¥ï¼ˆå¦‚æœä¸ºNoneåˆ™é‡æ–°è®¡ç®—ï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        print("  ğŸ§  [åˆ†ç¦»æ­¥éª¤2] ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥ç”Ÿæˆæ–‡æœ¬...")

        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—åµŒå…¥ï¼Œåˆ™å…ˆç¼–ç 
        if precomputed_embeds is None:
            precomputed_embeds = self.encode_image_separately(image)

        # æ„å»ºè¾“å…¥
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        # åº”ç”¨å¯¹è¯æ¨¡æ¿
        text = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥ï¼ˆä¸åŒ…æ‹¬å›¾åƒï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥ï¼‰
        inputs = self.processor(
            text=[text],
            images=[image],
            return_tensors="pt",
            padding=True
        )

        # ç§»åˆ°GPU
        inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                 for k, v in inputs.items()}

        # ä½¿ç”¨ä¸åŸå§‹æµ‹è¯•ç›¸åŒçš„ç¡®å®šæ€§å‚æ•°ï¼ˆè´ªå©ªè§£ç ï¼‰
        with torch.no_grad():
            # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
            torch.manual_seed(42)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(42)

            # è´ªå©ªè§£ç ç”Ÿæˆï¼ˆå®Œå…¨ç¡®å®šæ€§ï¼‰
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # å…³é—­é‡‡æ ·ï¼Œä½¿ç”¨è´ªå©ªè§£ç 
                temperature=None,  # è´ªå©ªè§£ç ä¸éœ€è¦temperature
                num_beams=1,  # ä¸ä½¿ç”¨beam search
                pad_token_id=self.processor.tokenizer.pad_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id
            )

            # è§£ç è¾“å‡º
            input_len = inputs["input_ids"].shape[1]
            generated_ids = outputs[:, input_len:]
            generated_text = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )[0]

        print(f"    âœ“ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)} å­—ç¬¦")
        return generated_text

    def run_inference_tests(self, images: List[Image.Image], prompts: List[str]) -> Dict[str, Any]:
        """è¿è¡Œåˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯•"""
        print("ğŸ§ª è¿è¡Œåˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯•...")

        results = {
            "model_type": "separated",
            "model_info": {
                "seed": 42,
                "num_images": len(images),
                "num_prompts": len(prompts),
                "architecture": "separated_vit_encoder + llm_generator"
            },
            "tests": []
        }

        # é¢„å…ˆç¼–ç æ‰€æœ‰å›¾åƒ
        all_image_embeddings = {}
        print("\nğŸ“¸ é¢„ç¼–ç æ‰€æœ‰å›¾åƒ...")
        for img_idx, image in enumerate(images):
            print(f"  ç¼–ç å›¾ç‰‡ {img_idx+1}/{len(images)}")
            try:
                embeddings = self.encode_image_separately(image)
                all_image_embeddings[img_idx] = embeddings
            except Exception as e:
                print(f"    âŒ ç¼–ç å¤±è´¥: {str(e)}")
                all_image_embeddings[img_idx] = None

        # å¯¹æ¯ä¸ªå›¾åƒå’Œæç¤ºè¯ç»„åˆè¿è¡Œæµ‹è¯•
        print("\nğŸ¯ å¼€å§‹ç”Ÿæˆæµ‹è¯•...")
        for img_idx, image in enumerate(images):
            image_embeddings = all_image_embeddings.get(img_idx)

            if image_embeddings is None:
                print(f"  âš ï¸ è·³è¿‡å›¾ç‰‡ {img_idx+1}ï¼ˆç¼–ç å¤±è´¥ï¼‰")
                continue

            for prompt_idx, prompt in enumerate(prompts):
                print(f"\næµ‹è¯•å›¾ç‰‡ {img_idx+1}/{len(images)}, æç¤ºè¯ {prompt_idx+1}/{len(prompts)}")
                print(f"  æç¤ºè¯: {prompt[:50]}...")

                try:
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ é€’åŸå§‹å›¾åƒï¼Œå› ä¸ºæ¨¡å‹ä»éœ€è¦å®Œæ•´çš„è¾“å…¥æµç¨‹
                    # è™½ç„¶æˆ‘ä»¬é¢„è®¡ç®—äº†åµŒå…¥ï¼Œä½†åœ¨å½“å‰å®ç°ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ˜¯éªŒè¯åˆ†ç¦»æµç¨‹æ˜¯å¦å¯è¡Œ
                    generated_text = self.generate_with_precomputed_embeds(
                        prompt=prompt,
                        image=image,
                        precomputed_embeds=image_embeddings,
                        max_new_tokens=150
                    )

                    # è®¡ç®—è¾“å‡ºå“ˆå¸Œç”¨äºå¯¹æ¯”
                    output_hash = hashlib.md5(generated_text.encode()).hexdigest()

                    embedding_shape = "unknown"
                    if image_embeddings.get("visual_features") is not None:
                        embedding_shape = str(image_embeddings["visual_features"].shape)

                    test_result = {
                        "image_id": img_idx + 1,
                        "prompt_id": prompt_idx + 1,
                        "prompt": prompt,
                        "generated_text": generated_text,
                        "output_hash": output_hash,
                        "token_count": len(self.processor.tokenizer.encode(generated_text)),
                        "embedding_shape": embedding_shape
                    }

                    results["tests"].append(test_result)

                except Exception as e:
                    print(f"    âŒ æ¨ç†å¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()

                    # è®°å½•å¤±è´¥çš„æµ‹è¯•
                    test_result = {
                        "image_id": img_idx + 1,
                        "prompt_id": prompt_idx + 1,
                        "prompt": prompt,
                        "generated_text": f"ERROR: {str(e)}",
                        "output_hash": "error",
                        "token_count": 0,
                        "embedding_shape": "error"
                    }
                    results["tests"].append(test_result)

        print("\nâœ… åˆ†ç¦»æ¨¡å‹æ¨ç†æµ‹è¯•å®Œæˆ")
        return results

    def save_results(self, results: Dict[str, Any], filename: str = "separated_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹åˆ†ç¦»Qwen2-VLæ¨ç†æµ‹è¯•...")

        # åŠ è½½æµ‹è¯•æ•°æ®
        images = self.load_test_images()
        prompts = self.get_test_prompts()

        # è¿è¡Œæ¨ç†æµ‹è¯•
        results = self.run_inference_tests(images, prompts)

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        # æ˜¾ç¤ºæ‘˜è¦
        self.print_summary(results)

        return results

    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("åˆ†ç¦»æ¨¡å‹æµ‹è¯•æ‘˜è¦")
        print("="*60)
        print(f"æ¨¡å‹ç±»å‹: {results['model_type']}")
        print(f"æ¶æ„: {results['model_info']['architecture']}")
        print(f"ç§å­: {results['model_info']['seed']}")
        print(f"æµ‹è¯•å›¾ç‰‡æ•°: {results['model_info']['num_images']}")
        print(f"æµ‹è¯•æç¤ºè¯æ•°: {results['model_info']['num_prompts']}")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(results['tests'])}")

        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        successful = [t for t in results['tests'] if t['output_hash'] != 'error']
        failed = [t for t in results['tests'] if t['output_hash'] == 'error']

        print(f"æˆåŠŸ: {len(successful)}, å¤±è´¥: {len(failed)}")

        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœç¤ºä¾‹
        print("\nç¤ºä¾‹ç»“æœ:")
        for i, test in enumerate(successful[:3]):
            print(f"\næµ‹è¯• {i+1}:")
            print(f"  å›¾ç‰‡ID: {test['image_id']}")
            print(f"  æç¤ºè¯: {test['prompt'][:50]}...")
            print(f"  ç”Ÿæˆé•¿åº¦: {len(test['generated_text'])} å­—ç¬¦")
            print(f"  è¾“å‡ºå“ˆå¸Œ: {test['output_hash'][:16]}...")
            print(f"  ç”Ÿæˆæ–‡æœ¬: {test['generated_text'][:100]}...")


def compare_results(original_file: str = "original_results.json",
                   separated_file: str = "separated_results.json"):
    """å¯¹æ¯”åŸå§‹å’Œåˆ†ç¦»æ¨¡å‹çš„ç»“æœ"""
    print("\n" + "="*80)
    print("ç»“æœä¸€è‡´æ€§å¯¹æ¯”åˆ†æ")
    print("="*80)

    # åŠ è½½ç»“æœ
    try:
        with open(original_file, 'r', encoding='utf-8') as f:
            original_results = json.load(f)
        print(f"âœ… å·²åŠ è½½åŸå§‹ç»“æœ: {original_file}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°åŸå§‹ç»“æœæ–‡ä»¶: {original_file}")
        print("è¯·å…ˆè¿è¡Œ original_qwen2vl_test.py")
        return

    try:
        with open(separated_file, 'r', encoding='utf-8') as f:
            separated_results = json.load(f)
        print(f"âœ… å·²åŠ è½½åˆ†ç¦»ç»“æœ: {separated_file}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°åˆ†ç¦»ç»“æœæ–‡ä»¶: {separated_file}")
        return

    # å¯¹æ¯”åˆ†æ
    original_tests = {(t["image_id"], t["prompt_id"]): t for t in original_results["tests"]}
    separated_tests = {(t["image_id"], t["prompt_id"]): t for t in separated_results["tests"]}

    print(f"\nğŸ“Š å¯¹æ¯”ç»Ÿè®¡:")
    print(f"åŸå§‹æ¨¡å‹æµ‹è¯•æ•°: {len(original_tests)}")
    print(f"åˆ†ç¦»æ¨¡å‹æµ‹è¯•æ•°: {len(separated_tests)}")

    # è¯¦ç»†å¯¹æ¯”
    exact_matches = 0
    hash_matches = 0
    length_differences = []

    comparison_results = []

    for key in original_tests:
        if key not in separated_tests:
            print(f"âš ï¸ åˆ†ç¦»æ¨¡å‹ç¼ºå¤±æµ‹è¯•ç”¨ä¾‹: å›¾ç‰‡{key[0]}, æç¤ºè¯{key[1]}")
            continue

        orig = original_tests[key]
        sep = separated_tests[key]

        # è·³è¿‡é”™è¯¯çš„æµ‹è¯•
        if orig.get("output_hash") == "error" or sep.get("output_hash") == "error":
            continue

        # æ–‡æœ¬å®Œå…¨ä¸€è‡´
        text_match = orig["generated_text"] == sep["generated_text"]
        # å“ˆå¸Œä¸€è‡´
        hash_match = orig["output_hash"] == sep["output_hash"]
        # é•¿åº¦å·®å¼‚
        length_diff = abs(len(orig["generated_text"]) - len(sep["generated_text"]))

        if text_match:
            exact_matches += 1
        if hash_match:
            hash_matches += 1

        length_differences.append(length_diff)

        comparison_results.append({
            "image_id": key[0],
            "prompt_id": key[1],
            "prompt": orig["prompt"][:50] + "...",
            "text_match": text_match,
            "hash_match": hash_match,
            "length_diff": length_diff,
            "original_text": orig["generated_text"][:100] + "...",
            "separated_text": sep["generated_text"][:100] + "..."
        })

    # ç»Ÿè®¡ç»“æœ
    valid_tests = len(comparison_results)
    if valid_tests > 0:
        exact_match_rate = (exact_matches / valid_tests) * 100
        hash_match_rate = (hash_matches / valid_tests) * 100
        avg_length_diff = sum(length_differences) / len(length_differences) if length_differences else 0

        print(f"\nğŸ“ˆ ä¸€è‡´æ€§åˆ†æç»“æœ:")
        print(f"æœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹: {valid_tests}")
        print(f"å®Œå…¨åŒ¹é…ç‡: {exact_match_rate:.2f}% ({exact_matches}/{valid_tests})")
        print(f"å“ˆå¸ŒåŒ¹é…ç‡: {hash_match_rate:.2f}% ({hash_matches}/{valid_tests})")
        print(f"å¹³å‡é•¿åº¦å·®å¼‚: {avg_length_diff:.2f} å­—ç¬¦")

        # æ˜¾ç¤ºä¸åŒ¹é…çš„æƒ…å†µ
        if exact_matches < valid_tests:
            print(f"\nâš ï¸ å‘ç° {valid_tests - exact_matches} ä¸ªä¸å®Œå…¨åŒ¹é…çš„æ¡ˆä¾‹:")
            mismatch_count = 0
            for comp in comparison_results:
                if not comp["text_match"] and mismatch_count < 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"\næ¡ˆä¾‹ {comp['image_id']}-{comp['prompt_id']}:")
                    print(f"  æç¤ºè¯: {comp['prompt']}")
                    print(f"  åŸå§‹è¾“å‡º: {comp['original_text']}")
                    print(f"  åˆ†ç¦»è¾“å‡º: {comp['separated_text']}")
                    print(f"  é•¿åº¦å·®å¼‚: {comp['length_diff']} å­—ç¬¦")
                    mismatch_count += 1

        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_summary = {
            "total_tests": valid_tests,
            "exact_matches": exact_matches,
            "exact_match_rate": exact_match_rate,
            "hash_matches": hash_matches,
            "hash_match_rate": hash_match_rate,
            "average_length_difference": avg_length_diff,
            "detailed_comparisons": comparison_results
        }

        with open("comparison_results.json", 'w', encoding='utf-8') as f:
            json.dump(comparison_summary, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: comparison_results.json")

        # ç»“è®º
        if exact_match_rate > 95:
            print("\nğŸ‰ ç»“è®º: åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†é«˜åº¦ä¸€è‡´ï¼")
        elif exact_match_rate > 80:
            print("\nâœ… ç»“è®º: åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†åŸºæœ¬ä¸€è‡´")
        else:
            print("\nâš ï¸ ç»“è®º: åˆ†ç¦»æ¨ç†ä¸åŸå§‹æ¨ç†å­˜åœ¨æ˜æ˜¾å·®å¼‚")
            print("ğŸ’¡ æç¤º: ç”±äºéšæœºé‡‡æ ·çš„å½±å“ï¼Œå®Œå…¨ä¸€è‡´æ€§å¯èƒ½éš¾ä»¥è¾¾åˆ°")
            print("ğŸ’¡ å»ºè®®: å¯ä»¥ä½¿ç”¨greedy decoding (temperature=0) æ¥æé«˜ä¸€è‡´æ€§")
    else:
        print("\nâŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹å¯ä¾›å¯¹æ¯”")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""

    # é…ç½®æ¨¡å‹è·¯å¾„
    MODEL_PATH = "Qwen/Qwen2-VL-2B-Instruct"

    try:
        # åˆå§‹åŒ–åˆ†ç¦»æµ‹è¯•å™¨
        tester = SeparatedQwen2VLTester(
            model_path=MODEL_PATH,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = tester.run_full_test()

        print("\nğŸ‰ åˆ†ç¦»æ¨¡å‹æµ‹è¯•å®Œæˆï¼")

        # è‡ªåŠ¨è¿›è¡Œç»“æœå¯¹æ¯”
        print("\nğŸ” å¼€å§‹å¯¹æ¯”åŸå§‹å’Œåˆ†ç¦»æ¨¡å‹ç»“æœ...")
        compare_results()

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
