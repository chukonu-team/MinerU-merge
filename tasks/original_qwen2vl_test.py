"""
åŸå§‹Qwen2-VLæ¨¡å‹æµ‹è¯• (å®Œæ•´æ¨ç†)
ç”¨äºå¯¹æ¯”éªŒè¯åˆ†ç¦»æ¨ç†çš„ä¸€è‡´æ€§
"""

import torch
import vllm
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import json
import hashlib
from typing import List, Dict, Any
import os

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
def set_seeds(seed: int = 42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    # vLLMçš„éšæœºç§å­é€šè¿‡sampling_paramsè®¾ç½®

class OriginalQwen2VLTester:
    """åŸå§‹Qwen2-VLå®Œæ•´æ¨ç†æµ‹è¯•å™¨"""

    def __init__(self, model_path: str, **vllm_kwargs):
        """
        åˆå§‹åŒ–åŸå§‹æ¨¡å‹æµ‹è¯•å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            **vllm_kwargs: vLLMå‚æ•°
        """
        print("ğŸ”¥ åˆå§‹åŒ–åŸå§‹Qwen2-VLæ¨¡å‹...")

        # è®¾ç½®ç§å­
        set_seeds(42)

        # åˆå§‹åŒ–å®Œæ•´çš„vLLMå®ä¾‹
        self.llm = vllm.LLM(
            model=model_path,
            seed=42,  # vLLMç§å­
            **vllm_kwargs
        )

        self.tokenizer = self.llm.get_tokenizer()
        print("âœ… åŸå§‹æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def create_test_images(self) -> List[Image.Image]:
        """åˆ›å»ºæ ‡å‡†æµ‹è¯•å›¾åƒ"""
        images = []

        # å›¾åƒ1: çº¢è‰²æ­£æ–¹å½¢
        img1 = Image.new('RGB', (336, 336), color='red')
        draw1 = ImageDraw.Draw(img1)
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        draw1.text((50, 150), "RED SQUARE", fill='white', font=font)
        images.append(img1)

        # å›¾åƒ2: è“è‰²åœ†å½¢
        img2 = Image.new('RGB', (336, 336), color='lightblue')
        draw2 = ImageDraw.Draw(img2)
        draw2.ellipse([68, 68, 268, 268], fill='blue')
        draw2.text((120, 150), "BLUE", fill='white', font=font)
        images.append(img2)

        # å›¾åƒ3: å½©è‰²æ¡çº¹
        img3 = Image.new('RGB', (336, 336), color='white')
        draw3 = ImageDraw.Draw(img3)
        colors = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']
        for i, color in enumerate(colors):
            y = i * 56
            draw3.rectangle([0, y, 336, y+56], fill=color)
        images.append(img3)

        # ä¿å­˜æµ‹è¯•å›¾åƒ
        os.makedirs("test_images", exist_ok=True)
        for i, img in enumerate(images):
            img.save(f"test_images/test_image_{i+1}.png")

        print(f"âœ… åˆ›å»ºäº† {len(images)} å¼ æµ‹è¯•å›¾åƒ")
        return images

    def get_test_prompts(self) -> List[str]:
        """è·å–æ ‡å‡†æµ‹è¯•æç¤ºè¯"""
        prompts = [
            "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶å’Œæ–‡å­—ã€‚",
            "è¿™å¼ å›¾ç‰‡çš„ä¸»è¦é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå‡ ä½•å½¢çŠ¶ï¼Ÿ",
            "å¦‚æœè¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªlogoï¼Œå®ƒå¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
            "ç”¨ä¸€å¥è¯æ€»ç»“è¿™å¼ å›¾ç‰‡çš„ç‰¹å¾ã€‚",
        ]
        return prompts

    def run_inference_tests(self, images: List[Image.Image], prompts: List[str]) -> Dict[str, Any]:
        """è¿è¡ŒåŸå§‹æ¨¡å‹æ¨ç†æµ‹è¯•"""
        print("ğŸ§ª è¿è¡ŒåŸå§‹æ¨¡å‹æ¨ç†æµ‹è¯•...")

        results = {
            "model_type": "original",
            "model_info": {
                "seed": 42,
                "num_images": len(images),
                "num_prompts": len(prompts)
            },
            "tests": []
        }

        # å›ºå®šçš„ç”Ÿæˆå‚æ•°ç¡®ä¿å®Œå…¨ä¸€è‡´æ€§ï¼ˆè´ªå©ªè§£ç ï¼‰
        sampling_params = vllm.SamplingParams(
            temperature=0.0,  # å®Œå…¨ç¡®å®šæ€§ï¼ˆè´ªå©ªè§£ç ï¼‰
            top_p=1.0,  # ä¸ä½¿ç”¨nucleus sampling
            max_tokens=150,
            seed=42,  # è®¾ç½®ç§å­ï¼ˆè´ªå©ªè§£ç ä¸‹ä¸éœ€è¦ï¼Œä½†ä¿ç•™ï¼‰
            stop_token_ids=None
        )

        # éœ€è¦ä½¿ç”¨processorå¤„ç†å›¾åƒ
        from transformers import Qwen2VLProcessor
        processor = Qwen2VLProcessor.from_pretrained(self.llm.llm_engine.model_config.model)

        for img_idx, image in enumerate(images):
            for prompt_idx, prompt in enumerate(prompts):
                print(f"  æµ‹è¯•å›¾ç‰‡ {img_idx+1}/{len(images)}, æç¤ºè¯ {prompt_idx+1}/{len(prompts)}")

                # ä½¿ç”¨processorå¤„ç†è¾“å…¥
                conversation = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]

                # å¤„ç†å¯¹è¯
                text = processor.apply_chat_template(
                    conversation,
                    tokenize=False,
                    add_generation_prompt=True
                )

                # å¤„ç†å›¾åƒå’Œæ–‡æœ¬
                inputs = processor(
                    text=text,
                    images=[image],
                    return_tensors="pt"
                )

                # æå–multimodalæ•°æ® - éœ€è¦å°†tensorè½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                mm_data = {}
                if "pixel_values" in inputs:
                    mm_data["pixel_values"] = inputs["pixel_values"]
                if "image_grid_thw" in inputs:
                    mm_data["image_grid_thw"] = inputs["image_grid_thw"]

                # æ‰§è¡Œæ¨ç† - ä½¿ç”¨æ­£ç¡®çš„vLLM APIä¼ é€’å¤šæ¨¡æ€æ•°æ®
                from vllm.inputs import TextPrompt
                prompt_input = TextPrompt(
                    prompt=text,
                    multi_modal_data={"image": image}
                )

                outputs = self.llm.generate(
                    prompt_input,
                    sampling_params=sampling_params
                )

                # æå–ç»“æœ
                generated_text = outputs[0].outputs[0].text.strip()

                # è®¡ç®—è¾“å‡ºå“ˆå¸Œç”¨äºå¯¹æ¯”
                output_hash = hashlib.md5(generated_text.encode()).hexdigest()

                test_result = {
                    "image_id": img_idx + 1,
                    "prompt_id": prompt_idx + 1,
                    "prompt": prompt,
                    "generated_text": generated_text,
                    "output_hash": output_hash,
                    "token_count": len(self.tokenizer.encode(generated_text)),
                    "finish_reason": outputs[0].outputs[0].finish_reason
                }

                results["tests"].append(test_result)
                print(f"    ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)} å­—ç¬¦")

        print("âœ… åŸå§‹æ¨¡å‹æ¨ç†æµ‹è¯•å®Œæˆ")
        return results

    def save_results(self, results: Dict[str, Any], filename: str = "original_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹åŸå§‹Qwen2-VLå®Œæ•´æ¨ç†æµ‹è¯•...")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        images = self.create_test_images()
        prompts = self.get_test_prompts()

        # è¿è¡Œæ¨ç†æµ‹è¯•
        results = self.run_inference_tests(images, prompts)

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        # æ˜¾ç¤ºæ‘˜è¦
        self.print_summary(results)

        return results

    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("åŸå§‹æ¨¡å‹æµ‹è¯•æ‘˜è¦")
        print("="*60)
        print(f"æ¨¡å‹ç±»å‹: {results['model_type']}")
        print(f"ç§å­: {results['model_info']['seed']}")
        print(f"æµ‹è¯•å›¾ç‰‡æ•°: {results['model_info']['num_images']}")
        print(f"æµ‹è¯•æç¤ºè¯æ•°: {results['model_info']['num_prompts']}")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(results['tests'])}")

        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœç¤ºä¾‹
        print("\nç¤ºä¾‹ç»“æœ:")
        for i, test in enumerate(results['tests'][:3]):
            print(f"\næµ‹è¯• {i+1}:")
            print(f"  å›¾ç‰‡ID: {test['image_id']}")
            print(f"  æç¤ºè¯: {test['prompt'][:50]}...")
            print(f"  ç”Ÿæˆé•¿åº¦: {len(test['generated_text'])} å­—ç¬¦")
            print(f"  è¾“å‡ºå“ˆå¸Œ: {test['output_hash'][:16]}...")
            print(f"  ç”Ÿæˆæ–‡æœ¬: {test['generated_text'][:100]}...")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""

    # é…ç½®æ¨¡å‹è·¯å¾„ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    MODEL_PATH = "Qwen/Qwen2-VL-2B-Instruct"

    try:
        # åˆå§‹åŒ–æµ‹è¯•å™¨
        tester = OriginalQwen2VLTester(
            model_path=MODEL_PATH,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            enforce_eager=True  # ç¡®ä¿ç»“æœä¸€è‡´æ€§
        )

        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = tester.run_full_test()

        print("\nğŸ‰ åŸå§‹æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
        print("è¯·ç»§ç»­è¿è¡Œåˆ†ç¦»æ¨¡å‹æµ‹è¯• (separated_qwen2vl_test.py) è¿›è¡Œå¯¹æ¯”")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()